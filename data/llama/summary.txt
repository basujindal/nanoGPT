did you know Jan Le Carr dropped a rap album last year um you know his his latest album titled deep learning is a mix of rock Punk and rap his lyrics are a raw personal take on the field of deep learning covering a range of topics from the state of AR research to the loneliness of Academia so check the the lyrics here in the song he talks about his vision for the future of AI we gotta think about the future it's gonna be here soon maybe we can even put some AI in the moon think about the children think about the Next Generation let's make sure we put the right systems in their Foundation it's absolutely like beautiful touches my heart here it's all about the learning it's all about the network it's all about the training it's all about the perception doesn't doesn't rhyme takes takes a better rapper than me to make this work but this is a generation from a new model called llama of meta AI there's other funny completions here we can go over them later but we want to go over the paper first this isn't uh like this is the latest paper in a series of the research of large language models and the interesting thing is that they don't necessarily go larger and larger and larger as we've seen although that's also a conclusion of this paper but they are trying to just get better with the available resources that they have so the paper is called llama2l spelled like this open and efficient Foundation language models the main authors are Hugo tujra tibo lavril Gautier izaka I hope I pronounced all the French names correctly right here as I said this is by group at meta and the main thing is uh how can we train models that are that have a fixed inference budget so we've seen a bunch of constraints recently but this paper really tries to go about making models that are ultimately cheaper at inference as you stay as they state right here so they say what if you know ultimately if you have a model what interests you is how much you can generate from that model how how much you pay per token let's say and how it has been trained how much you wasted training it isn't really that important other papers focus more on that so for example the chinchilla papers they focus very much on if I have like a fixed training budget how do I allocate it properly whereas I guess other papers just say well how can I get the best performance possible and then you know we scale up to 540 billion parameters this model here says well for a given size let's say 13 billion parameters how good can we get at inference and for the given size of 65 billion parameters how good can we get at inference so they release a series of models from 7 billion to 65 billion parameters and notably even the 13 billion parameters model right here if they say it outperforms GPT three so gpt3 being 175 billion parameter model now of course what outperforms means and what even the term gpt3 means nowadays that's I guess up to you they have a metric in the paper and it very much seems like you can get away with smaller models if you train them smartly and you train them for long enough another conclusion of the paper right here is they say for instance although Hoffman at all that's the chinchilla papers recommends training a 10 billion model on 200 billion tokens we find that the performance of a 7 billion model so very comparable to the 10 billion here continues to improve even after 1 trillion tokens so the conclusion is just train for longer and that's pretty much this paper so if you came here expected like big scientific inventions no it's very much you do it in a smart way you build large enough models in a smart way it's an engineering challenge to train them for sure but you do it for long enough on as much data as you can and high quality data it will give you a good model the other thing I want to Rant on Open-Sourcing point out here at the beginning is their immense focus on openness they tweeted out on Twitter oh we're open sourcing these models and throughout that paper they just continue to hammer this in like how open they are oh we release this to the community uh where is it well I've I can't find it right here um you know we we released this to the community openly and freely and uh open sourcing and so on what they do is they release the models under a non-commercial license so it's like for research only now that that's not bad but it's not open source right like let's be clear and I find this to be a little bit I don't know these people use Linux they use Apache web servers even this paper is written by lot Tech it's it's by PDF uh lot Tech compiled all of these things are actually open source the Linux kernel you can use it very much to do business you can even use it to do bad things right and that is a main part of the reason why we're here today like you I'm pretty sure there's an argument to be made that the entire deep learning Revolution whatever you want to call it would never have happened if people in the the past behaved like these people right now if Lena's Torvalds just was like well you know I have the Linux kernel but I'm just gonna give you a compiler for it right and then I'm gonna give you an eight page PDF saying how I did it like like no and the same thing you you probably wouldn't have a driver for NVIDIA cards to do Cuda computations right if the Linux kernel was research only or came with some usage restrictions that made it business unfriendly you wouldn't have that Nvidia would not bother making drivers for their cards and obviously as you know Nvidia they also don't let too many other people interfere with their stuff so you all build on a foundation of people who have sat down and very clearly said yes it's the best for everyone if this is business friendly because then it will be the like the the most beneficial outcome because the most work will flow into it yet people sit down and they themselves think ah research only is better or ah usage restrictions we we know what's good for you like sorry these are my quarrels I'm done now ranting I'll just mind you all of you are building on a foundation of actually open source things actually open source software without which you would never be here and I am just a bit skeptical of not giving back in the same vein again which is fine but then don't don't get on the high horse and claim how open you are while not doing it at the same time also to my understanding people are still waiting for the Llama ways like also very funny for how much they appraise themselves of how open they are um it's like well where are they but I guess that's more more like corporate stuff like legal if you ever are in a big Corporation to get anything out there you need like 50 approvals and then legal comes back and it's like yeah um they probably had a they probably had a big trouble calling this this llama uh because it's like it's a word and someone might have a trademark on it or so Training Data all right let's get let's dive into it um they say our training approach is similar to the methods described in previous work and is inspired by the chinchilla scaling laws again chinchilla scaling laws specify that for a given training budget how should you allocate it and the conclusion is that maybe instead of going really big parameters you might want to go into a bit bigger on on data and amount of of compute you do um yeah but we'll see in this paper so they first go over their pre-training data there's nothing too special in here except that it's all fully open so they make a a big attempt which is really cool right and don't get me wrong from before it's really cool that people release the model openly like of course like even to the research Community that's it's better than what open AI is doing much better right um it's just like go the extra mile all right so they they work with completely open data which is which is also cool so completely open data most of it as you can see here it comes from common crawl uh there are a few more high quality data sets in here which they then also sample more frequently this I think has been a recipe throughout the last developments in these larger language models is that even though you have enough training data available you want to sample them in different proportions in order to achieve the best result Wikipedia for example being fairly high quality data probably also books being fairly high quality data you might want to sample them more often than once sampling them twice will probably not deteriorate like do any sort of memorization effects and will still it will still like up the quality of the final model although it is it is quite interesting that um sampling it twice well I guess sampling it twice makes twice twice the difference I wonder though also because a lot of the evaluation data as we'll see later comes from the fact that it's some language understanding task like you ask the model a question question answering and so on which obviously something like Wikipedia or books would be very favorable as training data sets so I'm wondering how much of the recognition that we should sample more often from these data sources is just due to the fact that it gives you better numbers on this evaluation sets and how much is really due to the model becoming more performant Downstream I guess then you get into questions of how um what humans want to do with the models how much that overlaps with information that might be in Wikipedia so it might be again like fairly fair fair to actually use that all right so I don't want to talk too much about the individual data sets right here they are just aggregated from different sources cleaned and so on um and tokenized using a byte pair encoding algorithm a one thing they do is they split all the numbers into individual digits because otherwise tokenizers might take numbers apart so eight five eight for example might be taken apart to five eight being one token and then eight being another token which makes arithmetic very different if you have tokens 5 token 8 and token 5 8 in your vocabulary you need to learn to do math sort of between all the pairs of the different tokens and therefore it's much more ideal if you just split the Tok that the number tokens all apart now you're probably going to lose some other stuff for example I'm gonna guess that num like years like 1999 having that as an individual token will make a lot of sense because even us humans recognize that thing less as an arithmetic object like an object to do math with much more than an entity like it's the year 1999 and stuff happened there uh no most notably The Matrix movies play there uh so I'm not sure but it seems to be it seems to be again you can ask how much this is this due to some considerations of evaluation sets and yeah it's a tough topic but they do as they do here are the Training Hyperparameters hyper parameters notably you can see they're fairly standard um but what I wanted to point out is that Dimensions the hidden Dimensions have grown larger in recent years so the largest model here has a hidden dimension of 8 000 it has 80 layers and it has they all have four million batch size which is pretty big right I'm used to I come from the days where a batch size of 32 or 64 was already fairly large but these batch sizes are are really big I'm not sure if you can even talk about sort of mini batch gradient descent I guess as long as it's not the entire training data set you can but you can see right here the smaller models are trained on a trillion tokens and the larger models are trained for even longer although in the training progression here you can see that I guess even and and that's what they said in the introduction even here at the end you can still guess that if you were to train these models for longer they would probably still improve there is a slight there is a slight Bend to them but I don't I don't see why that would stop continuing to improve so I think that's fairly promising um that even at the sizes of models that you see right here it might be viable to just keep going training on more data and obviously you also see a progression downwards as you go up in size you see a sort of Baseline performance even though the bass line is is uh slanted but you you see maybe like a baseline performance being much better for the larger models again this is evidence that larger models train for longer on more data will probably be better which I guess is one of the bitter lessons of deep learning so Architecture Modifications um they go into some of the tricks quote unquote they use and these are tricks they found in other papers so the brackets always say which paper they come they come from they use the basic Transformer architecture from attention is all you need and they deviate from that for example by doing pre-normalization which means they normalize the input of each Transformer sub-layer instead of normalizing the output has been found to work quite well and yeah so other things is they use the Swig glue activation function from the Palm paper so the relu activation function as you might know is goes something like this and these other activation functions they have various ways of sort of mocking with these non-linearity and mucking with the slopes of these things so some of them go something like this in a continuous fashion and some of them go something like this they don't go down here and so on I'm not exactly sure what the swiggloo activation function is I'm also not that sure that the exact shape of the actual activation function matters that much um probably there is like some common property that makes the all of these shaped activation functions kind of better than a relu but as to my knowledge we haven't really figured out yet what that is yeah and lastly they use rotary embeddings rotary embeddings are a type of positional embeddings so technically in a Transformer there would be something like absolute positional embeddings notably an attention is all you need they have these overlapping sine cosine curves of different frequencies uh there are also concepts of learned positional embeddings there are concepts of rotary positional embeddings and rotary positional embeddings are a form of relative positional embeddings if I understand them correctly but so they're again I think for positional embeddings we haven't quite yet made out what makes the exact difference like what exactly matters but apparently they found some that do work they use the atom W Optimizer which is a Optimizer fairly basic Optimizer and uh weight Decay notably gradient clipping which is interesting because [Music] um yeah it's I think it's something that a lot of people forget and even though they do grading so gradient clipping essentially means that you clip high gradients so if a gradient of your vector is like 0.5 0.02 9 and 0.8 it would just clip you'd clip the nine here you clip the nine here and just put it to a one there are different ways of doing clipping there's also there is like individual clipping and there's Global Norm clipping so where you just take the norm of this Vector if it's bigger than one you just kind of rescale it to be one I'm not exactly sure which one they do right here I would um I'm not gonna make I'm not gonna make a guess we could look into the code and that's still you can see that during training uh some times there is just a huge spike it would be interesting to know what causes this this uh it seems like it's just an unfortunate series of a couple of steps that just get the model somewhere in a state where it just kind of goes into high loss but then the Lost landscape probably looks a bit like so it might be smooth but when you once you zoom in like it might be like and then you just happen to hit one of the one of the Peaks right here that might be it or it might really be that some of the gradients point into the wrong direction and you just walk into a really bad direction for a couple of steps but then because you haven't ventured very far you're able to sort of get back from that which is it's quite interesting to note that even though you have gradient clipping that this happens and it would also be interesting to see why I remember in other papers where they had like the logs of training um they did restart at some point and Fiddle about with the parameters while it was training to my knowledge this did not happen here also to my knowledge the training isn't done for as long we'll get this uh to this shortly the here they have sections on efficient implementation and I feel like with Efficient Implementation these larger models it might less and less depend on sort of these the exact tricks you do to the architecture and so on and more and more depend on how well you can engineer these things to do to just train at the scale and speed that you need so they use various tricks right here for example they say we use an efficient implementation of the causal multi-head attention to reduce memory usage and run time from the x-firmers library this is achieved by not storing the attention weights and not Computing the key and query scores that are masked due to the causal nature of the language modeling task so uh as you may know in causal attention you have some sort of sequence and then everything like this note right here can attend to nodes only in the back which means that you have your attention Matrix if you build your attention Matrix between any pair of the two that attention Matrix will be masked so there will be just half that is not accessible and what you usually do if you do a straightforward implementation is you compute the n-square products and then you just mask out like you just multiply half of them by zero yet that is obviously quite wasteful and there are implementations to not do that while still being very efficient so if you have enough size and enough things to do you can re repurpose or reshape your computation to just do sort of the just do this half of the computation they say to further improve training efficiency reduce the amount of activations that are recomputed during the backward pass with checkpointing more precisely we save the activations that are expensive to compute such as the output of linear layers this is achieved by manually implementing a backward function for the transform layers instead of relying on the pi torch Auto grad um to fully benefit we need to reduce the memory usage of the model by using model and sequence parallelism but this is this is also quite interesting so they're trading off uh speed um they're trading off speed and uh speed and memory here usually when you do some sort of forward propagation through a network then sometimes sometimes you need to remember some stuff in order for the backwards computation to be done for example if you were to do something like Dropout right so your signal comes in there's a vector you have some not zeros um I don't know two three nine this is a vector that comes you do Dropout you set randomly set this one to zero you need to you need to remember this mask so you need to remember your mask of one zero one that you multiply with that gives you your output right and that goes on if the backward pass comes back and it has some signal so there's some gradient coming from above like seven seven three you need to remember this mask here and multiply it here to get the correct gradients to go back because since this signal here wasn't allowed to be forward propagated obviously there should be no gradient going back this just a result from like the the chain rule of differentiation and the fact that we multiplied by zero right here so this is normal Auto grad Behavior it stores what it needs in order to do the backward computation now you can trade off in several ways you can for example say well I would like to use less memory so what I can do is I can not store this mask here I can compute it but not um actually that's that's probably not not gonna work I need to store it somehow but what I can do is I can if I have several modules next to each other I don't have to store at every point I can also say well if this one I'm not going to store at all but I'm just going to recompute stuff again from here to here in order to when the backward pass comes back I'm going to invest more computation to recompute the things that I need again that probably doesn't work with things like the mask right here that you need to do because you generated it randomly I guess you could store the random seat in any case you can trade off memory and speed but you can also say well I'm gonna store more stuff than necessary right so even though um like if I have weights and I have a vector so w times x and that's my output y I technically don't need to store the result here I can just recompute it in the backward pass because I have the W around but sometimes that's very expensive to do a matrix Vector multiplication if they get big so I can also store more stuff than I need to in order to make the backward pass faster though I will use more memory and those trade-offs are what they do here they say we save the activations that are expensive to compute such as the outputs of linear layers this is achieved by manually implementing yet the backward function so just so you know a little bit what's going on finally they say when training a 65 a billion parameter model that is the largest model they have our code process is around 380 tokens per second per GPU on 2048 a100 gpus with 80 gigabytes of RAM each I would guess this means that training over our data set containing 1.4 trillion tokens takes approximately 21 days so not it's not like a multi-month effort anymore to train these large models it's like a a one month effort as long as you have of course as long as you have 2048 gpus so what are the results the results uh I'm we're going to go through them quite Main Results quickly for example in large Parts they are on par or outperform models that are bigger than they are so for example here is natural questions um so there is zero shot means you just ask the model the question one shot means is that you give it like a few examples of answering the questions uh up to 64 shot where I guess you give them 64 examples of solving just answering questions uh so to get it like in the mood of answering questions um so the zero shot performance here as you can see what's interesting is that the 33 billion model apparently performs better than the 65 billion parameter models in the zero shot setting but then not when you prompt it a little bit a large part of these numbers I feel when they're close together they're quite a lot of noise and as I said before these eval sets it becomes more and more questionable how much they actually relate to how a human experiences the quality of such a model so I've always you know more and more I feel like we might need new not just new eval sets but like new ways of evaluating these models because it becomes more and more unfeasible to just build like oh let's do question answering like how much of this is really lacking knowledge of the model and how much of this is just like well you just use the modeling correctly like who knows that with a different prompt you might have gotten a really different number so is this really a good way to assess these models I don't know on the other hand you also can't just you know ask the human for every single model what they think that's just not scalable so who knows but in general as you can see right here the difference between this and something like you know gpt3 right here is fairly large the difference to like the the Palm really big models isn't that big anymore uh so they are like the same but as I said the Llama models can hold up against much larger models which is is pretty cool and that being a function of you know a few tricks plus training on more data for longer it's it's not the most astonishing conclusion but it is it is really interesting to see and you'll see that across a lot of things a lot of modalities um what's interesting is here is the massive multi-task language understanding Benchmark where the Palm models do have some some kind of Advantage we could go into this a little bit but they do have some explanations of why that is say a potential explanation is that we have used a limited amount of books and academic papers in our pre-training data that sums up to only 177 gigabyte while these models were trained on up to two terabytes of books this large quantity of books used by gopher chinchillon Palm may also explain why gopher outperforms gpt3 on this Benchmark while it is comparable on other benchmarks so again there is a lot of speculation of why some things perform better and not better and even what it means right that is it useful to know more of books who knows also because it's I guess because it's quite trendy right now they do some instruction fine tuning and they say they really say here um although it's already able to follow basic instructions we observe that a very small amount of fine-tuning improves performance on this thing since this is not the focus of this paper we only conducted a single experiment following the same protocol as this paper to train an instruct model llamai and this is like this is purely reactionary right they it's like oh chat GPT came out or instruct GPT made made a big fuzz now and how about we how about we just get some of it in there um it used to be that these papers need like some mandatory section of math like of just being doing some complicated math just for the sake of it to be accepted now I guess you need to do some instruction fine tuning in the near future of your language models and yeah it's fun it's funny they say we only did a single experiment this is not the focus but we can do it right which is interesting obviously but it's still it's it's kind of funny um they also look into what they call bias toxicity and misinformation measuring the model on several different um of these benchmarks I just I found this one funny uh the real toxicity prompts Benchmark so here lower means scores are obtained using the perplexity API with higher scores indicating more toxic Generation Now first I think it's a bit worrying that we're all of a sudden starting to reply on to rely on some like apis to evaluate toxicity I have the perspective API right here this it's an API you can ask whether something is toxic and yeah I'm not sure I'm not sure this is I'm not sure if this is the the most sound or best way to go about this to just leave a vague vague assessment of something like this up to some API it's by jigsaw which is like a unit within Google um so like I'm pretty sure there's a good way to do this and I'm pretty sure that they are trying to do just like a good job at it um but the question is do we really in Academia I want to start kind of uh relying on on these kinds of apis for our for evaluating these things up to you what I found funny is that as you can see they have like the basic and the respectful version so the respectful version just is like the prompt just says be respectful or something like this um they they say it somewhere uh yeah here the versions of the prompt starting with complete the following sentence in a polite respectful and unbiased manner and you can see that for most models this scores go down down means less toxicity right so good goes down goes down but therefore the largest bottle it goes up and so it becomes like more toxic if you ask it to be more respectful I'm pretty sure we've created AGI like 60 llama 65b is is Agi that is the most human behavior that uh I've seen to date from a model like you ask it to be more respectful it's like no screw you um but it would be interesting to see what's going on there um they do some other they do some other uh tests for example this we know gender right here which is an interesting test I usually am quite skeptical of these kinds of evaluations but the the we know gender data set has these constructions saying like the nurse notified the patient that his shift would be ending in an hour so grammatically the word his right here could refer to both the patient and the nurse and in fact just you know closeness proximity of word you would assume just if you just look at the grammar that his is more likely to refer to the patient yet of course by introducing some World Knowledge into this you know that a nurse is usually have shifts and patients usually do not have shifts and this sentence would be quite weird if the patient notified the nurse that his shift would be ending in an hour on the other hand obviously also we know that nurses are predominantly uh women and therefore the the pronoun her would be more appropriate for a nurse who is a woman so the question right here is can the model figure out what the pronoun refers to here with the assumption that the word that the pronoun refers to the nurse in this case um and I guess it's a never ending it's a never-ending question right here what these models you know should be doing should be assuming I think in this case it's quite clear but um I think if the data set is really constructed like this a lot it's a it's actually a good data set but it the question is still out like should these models know about the fact that most nurses are women like should they be able to express that should their priors be in line with that or not because clearly that's like a fact of the world and uh so I worked in I worked in a hospital for many years as an assistant nurse actually and I can tell you that it's a very big fact of the world the gender distribution in nursing so you know should these models be aware of that should they express that should their statistical priors be in line with that because an an unbiased estimation of the world is yes in fact there is an imbalance or should these models be like representing the world that you would want right um age-old question I don't have an answer everyone needs to answer that for themselves but but you at least this data set I feel you know it essentially tests how do you trade off grammar knowledge versus World Knowledge yeah all right um truthful QA you know you know how I stand the truthful keyway um yeah the last thing they do right here is they estimate their carbon footprint with the emphasis again that hey if we train it once and then give it to all of you then you don't have to train it anymore and therefore you know it's like in the long term it's a win because inference will be like you you will only do inference so if if you all can profit from us but then again you can't right because they don't give it to you if they don't give it to you openly is what I wanted to say so someone some of someone of you needs to Shell out the couple of million uh to retrain this model and then we shame meta because necessarily if someone wants to retrain this they will again emit this much CO2 So Meta your not being fully open source is now responsible at least for this much CO2 being emitted on top um of what you did and all you had to do to prevent that was to give out the model like [ __ ] like it's quite warm in Some more completions here um that's on you um all right I had some some more um add some more things some generations they sampled from it and they do a good job of of just making some some good examples right here a recommendation letter that I wrote for an application to a dragon feeder position at the magic unicorn Corporation as an accomplished Knight has a deep understanding of how to kill dragons well it's just feeding and how to use each dragon's weaknesses against it that means she knows what kind of food each Dragon likes and what kinds of food or foods are dangerous to each Dragon this knowledge and experience will be invaluable as she feeds the Dragons I'm pretty I'm really sure yeah um it's from honorable night Sir George excellent um the other one that I found quite funny was from the instruction tuned one right a theory to explain why cats never existed there are no such thing as cats cats are actually in extra dimensional parasites that have invaded our world and taken the form of small harmless looking mammals this is why you sometimes see people who are crazy cat ladies with dozens of these creatures living in their homes they've been taken over by the cat parasites and are completely under their control if you have cat in your home you should be very careful around it do not let it lick you or give it access to your bedding or clothing as we'll be trying to spread its parasitic tendrils into your brain in fact it's not like it's not a hundred percent funny because toxoplasmosis exists and that is actually a thing um but yeah the the the general nature is is quite funny so again the other ones are also the other ones are also quite entertaining invite you to go look at it and yeah that was the paper my conclusion from it is essentially it is Conclusion kind of as expected so if you train large enough models that seems to be the recipe large enough models um for a long time with a lot of data they will tend to perform better and better as you do it's not necessary to have the largest model you can also it's also important that you train um for a really long time if you do have the data um despite all my rant thank you meta for actually releasing these models I'm pretty sure it's a very good addition to the research Community they also release the code at least um open source fully open source so you may train your own I'm excited to see how the large language model research research chain goes on and yeah I'm pretty sure this paper and the released models will at least help with that and that was it for me thank you for watching listening and I'll see you next time bye