{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import sys\n",
    "from llamaTokenizer import LLaMAtokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokenizer_path = \"/home/li/basu_workspace/llama/tokenizer.model\"\n",
    "tokenizer = LLaMAtokenizer(model_path=tokenizer_path)\n",
    "\n",
    "enc = lambda s: tokenizer.encode(s, bos=False, eos=True)\n",
    "dec = lambda s: tokenizer.decode(s)\n",
    "\n",
    "import json\n",
    "\n",
    "with open('/home/li/basu_workspace/nanoGPT/data/dolly/databricks-dolly-15k.jsonl') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "data = [json.loads(line) for line in data]\n",
    "data_cleaned  = [\"User: \" + instruct['instruction'] + \"\\nBot: \" + instruct['response'] for instruct in data]\n",
    "\n",
    "encoded = []\n",
    "for sentence in data_cleaned:\n",
    "    encoded.append(enc(sentence))\n",
    "assert len (encoded) == len(data_cleaned)\n",
    "\n",
    "seq_len = 2048\n",
    "### create new encoded if len(encoded[i]) < seq_len\n",
    "\n",
    "encoded = [encoded[i] for i in range(len(encoded)) if len(encoded[i]) < seq_len]\n",
    "\n",
    "comb = np.zeros((len(encoded), seq_len), dtype=np.int32)\n",
    "j = 0\n",
    "k = 0\n",
    "sen_lens = []\n",
    "l = []\n",
    "for i in encoded:\n",
    "    if k + len(i) > seq_len:\n",
    "        sen_lens.append(l)\n",
    "        l = [len(i)]\n",
    "        j+=1\n",
    "        k=len(i)\n",
    "    else:\n",
    "        k+=len(i)\n",
    "        l.append(len(i))\n",
    "    comb[j, k-len(i):k] = i\n",
    "\n",
    "for i in range(len(comb)):\n",
    "    if np.sum(comb[i]) == 0:\n",
    "        num_sen = i-1\n",
    "        break\n",
    "\n",
    "comb = comb[:num_sen]\n",
    "\n",
    "max_len = max([len(i) for i in sen_lens])\n",
    "## pad sen_lens with zeros\n",
    "sen_lens = [i + [0]*(max_len - len(i)) for i in sen_lens]\n",
    "sen_lens = [item for sublist in sen_lens for item in sublist]\n",
    "\n",
    "train_frac = 0.9\n",
    "train_ids = comb[:int(train_frac*len(comb))]\n",
    "val_ids = comb[int(train_frac*len(comb)):]\n",
    "\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "sen_lens = np.array(sen_lens, dtype=np.uint16)\n",
    "train_ids.tofile('/home/li/basu_workspace/nanoGPT/data/dolly/train.bin')\n",
    "val_ids.tofile( '/home/li/basu_workspace/nanoGPT/data/dolly/val.bin')\n",
    "sen_lens.tofile('/home/li/basu_workspace/nanoGPT/data/dolly/sen_lens.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(807, 2048)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.memmap('/home/li/basu_workspace/nanoGPT/data/dolly/val.bin', dtype=np.uint16, mode='r')\n",
    "train_data.shape\n",
    "train_data = train_data.reshape(-1, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is the best food in the world?\n",
      "Bot: Humans love their food, don’t they? Why is that? Well, humans are born in places where there is a local cuisine that they grow up with and love. Eating this food established an association with a comforting experience and this is why most nations claim that the best food is in their country. However, there seems to be a global understanding that Italian, French, Thai and Japanese cuisines are some of the best in the world. The reason for this, apart from top quality genuine ingredients and local cultural importance, can be the positive association attached to the cuisine by mass media or personal experiences whilst on holiday. User: Given a reference text about the wraparound process, tell me what is included in a wraparound plan.\n",
      "Bot: A wraparound plan usually includes formal services and interventions along with community services and interpersonal support. User: Can we time travel?\n",
      "Bot: By Einstein's relativity theory, time travel is possible. User: Who is the author of Atomic Habits?\n",
      "Bot: James Clear is the author of the bestselling book titled Atomic Habits. User: What is Daylight savings time?\n",
      "Bot: As the name suggests, Daylight savings time is a change in time during the spring and the fall.  The reasoning behind it was that people would enjoy having more daylight hours while awake. User: Who won the MLB World Series in 2020?\n",
      "Bot: Los Angeles Dodgers User: Why are over partitioned tables poor data layout strategy\n",
      "Bot: Over-partitioning creates physical boundaries beyond which the file compaction and z-ordering aren’t effective and you’re still left with the small files problems. With Delta, a better strategy is to use combination of partitioning and z-ordering to allow for better file layouts. \n",
      "\n",
      "Would be helpful to understand the business requirements & design details behind these highly partitioned tables you have in parquet today User: What is the national bird of New Zealand?\n",
      "Bot: The kiwi bird is the national bird of New Zealand.  This bird is particularly interesting because it cannot fly and it runs very fast.  If you see one in person consider yourself lucky because sightings are rare during the day and in highly populated areas.  It also has strong legs and no tail.  New Zealanders adore this bird and often refer to it as the 'humble kiwi.' User: What is DNA sequencing?\n",
      "Bot: DNA sequencing is the process of determining the nucleic acid sequence – the order of nucleotides in DNA. It includes any method or technology that is used to determine the order of the four bases: adenine, guanine, cytosine, and thymine. The advent of rapid DNA sequencing methods has greatly accelerated biological and medical research and discovery.\n",
      "\n",
      "Knowledge of DNA sequences has become indispensable for basic biological research, DNA Genographic Projects and in numerous applied fields such as medical diagnosis, biotechnology, forensic biology, virology and biological systematics. Comparing healthy and mutated DNA sequences can diagnose different diseases including various cancers, characterize antibody repertoire, and can be used to guide patient treatment. Having a quick way to sequence DNA allows for faster and more individualized medical care to be administered, and for more organisms to be identified and cataloged.\n",
      "\n",
      "The rapid speed of sequencing attained with modern DNA sequencing technology has been instrumental in the sequencing of complete DNA sequences, or genomes, of numerous types and species of life, including the human genome and other complete DNA sequences of many animal, plant, and microbial species.\n",
      "\n",
      "The first DNA sequences were obtained in the early 1970s by academic researchers using laborious methods based on two-dimensional chromatography. Following the development of fluorescence-based sequencing methods with a DNA sequencer, DNA sequencing has become easier and orders of magnitude faster.[ User: How do you score three points in basketball?\n",
      "Bot: You score three points by making a basket from beyond the three point line, which is an arc which is drawn around each basket. By sinking the ball from far away from the basket, you are rewarded with an extra point. User: How many people visit the zoo in the US each year?\n",
      "Bot: 181 million people in the US visit the zoo annually each year. User: Name the top scorer in NBA history.\n",
      "Bot: In 2023, Lebron James passed Kareem Abdul- Jabbar for most career points in NBA history. User: What is the best bike setup for Zwift's Three Little Sisters tour?\n",
      "Bot: I recommend a lightweight bike with good climbing capabilities for this route, such as the Specialized Tarmac, Canyon Ultimate, or Trek Emonda. These bikes offer a good balance of speed and climbing ability, which can be helpful on the steep ascents of the Three Little Sisters tour. The route will first tackle the Hilly KOM, Titan's Grove KOM, and finally, finish on the Volcano KOM. In terms of wheels, a set of lightweight climbing wheels such as the DT Swiss disc or Zipp 858's could be a good option, as they offer reduced weight and improved aerodynamics. User: Where did the word supercalifragalistic come from, and what does it mean?\n",
      "Bot: The word originated from the 1964 Disney musical film called Mary Poppins which was written by the Sherman Brothers, and sung by both Julie Andrews and Dick Van Dyke. The word was made up to as something to say when you really don't have anything to say, but it is also used in other instances when someone wants to describe something as being great or really good. User: Based on the following paragraph about the campuses of the University of Georgia, which types of trees can be found on the campus?\n",
      "Bot: On the campuses of the University of Georgia one can find native tree species such as magnolias, red oaks, white oaks, and beeches. There are also non-native tree species to be found such as the North Africa Atlas cedar, the Chinese parasol and royal paulownia, and the Japanese zelkova and black pine. User: What is a woodwind instrument?\n",
      "Bot: Woodwind instruments are a family of musical instruments that produce sound by splitting the air blown on a sharp edge, such as a reed. These instruments can be made of any materials, not just wood. User: Who created Doge Coin and when?\n",
      "Bot: Doge coin was created by Billy Markus and Jackson Palmer and it was introduced in December 6, 2013. User: From the passage provided, extract the year  Indian National Congress was founded\n",
      "Bot: Indian National Congress was founded in 1885 User: Identify which instrument is string or percussion: Hosho, Flamenco guitar\n",
      "Bot: Hosho is percussion, Flamenco guitar is string. User: Which season was Roger Federer's most important in his career?\n",
      "Bot: 2009 was arguably the most important of his career since he won his first and only French Open title, completing the career grand slam. He was also able to surpass Pete Sampras' record of fourteen men's grand slam titles.\n",
      "\n",
      "Until then, Federer had won the three other grand slams - Wimbledon, Australian Open and US Open but the French Open was elusive as he had to repeatedly compete on the clay court of Rolland Garros against Rafael Nadal, who was a clay court expert and he lost to him on multiple occasions.\n",
      "\n",
      "2004 was also an important year in Federer's career because he established dominance in the sport by winning three out of the four grand slams of the year and ending the year as the world number one for the first time in his long career. User: What is the downside of using mulch for your landscape?\n",
      "Bot: Mulch attracts termites and other insects. Mulch has to be replaced every other year. User: Queen Elizabeth II, who reigned UK for 70 years, passed away the age of 96 at\n",
      "Bot: Balmoral User: Which of these counties has only one neighbour, and which have more than one neighbour: Haiti, Russia, South Korea, Germany, Portugal, Ireland, Canada, Brazil, Denmark, Austria, Qatar and France.\n",
      "Bot: The following countries have only one neighbouring country: South Korea, Qatar, Portugal, Ireland, Haiti, Canada, and Denmark. These countries have multiple neighbours:Russia, Brazil, Germany, Austria, and France. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n"
     ]
    }
   ],
   "source": [
    "print(dec(train_data[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"User: When did Virgin Australia start operating?\\nBot: Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. User: Which is a species of fish? Tope or Rope\\nBot: Tope User: Why can camels survive for long without water?\\nBot: Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time. User: Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\\nBot: The name of the third daughter is Alice User: When was Tomoaki Komorida born?\\nBot: Tomoaki Komorida was born on July 10,1981. User: If I have more pieces at the time of stalemate, have I won?\\nBot: No. \\nStalemate is a drawn position. It doesn't matter who has captured more pieces or is in a winning position User: Given a reference text about Lollapalooza, where does it take place, who started it and what is it?\\nBot: Lollapalooze is an annual musical festival held in Grant Park in Chicago, Illinois. It was started in 1991 as a farewell tour by Perry Farrell, singe of the group Jane's Addiction. The festival includes an array of musical genres including alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. The festivals welcomes an estimated 400,000 people each year and sells out annually. Some notable headliners include: the Red Hot Chili Peppers, Chance the Rapper, Metallica, and Lady Gage. Lollapalooza is one of the largest and most iconic festivals in the world and a staple of Chicago. User: Who gave the UN the land in NY to build their HQ\\nBot: John D Rockerfeller User: Why mobile is bad for human\\nBot: We are always engaged one phone which is not good. User: Who was John Moses Browning?\\nBot: John Moses Browning is one of the most well-known designer of modern firearms.  He started building firearms in his father's shop at the age of 13, and was awarded his first patent when he was 24.\\n\\nHe  designed the first reliable automatic pistol, and the first gas-operated firearm, as well inventing or improving single-shot, lever-action, and pump-action rifles and shotguns.\\n\\nToday, he is most well-known for the M1911 pistol, the Browning Automatic Rifle, and the Auto-5 shotgun, all of which are in still in current production in either their original design, or with minor changes.  His M1911 and Hi-Power pistols designs are some of the most reproduced firearms in the world today. User: Who is Thomas Jefferson?\\nBot: Thomas Jefferson (April 13, 1743 – July 4, 1826) was an American statesman, diplomat, lawyer, architect, philosopher, and Founding Father who served as the third president of the United States from 1801 to 1809. Among the Committee of Five charged by the Second Continental Congress with authoring the Declaration of Independence, Jefferson was the Declaration's primary author. Following the American Revolutionary War and prior to becoming the nation's third president in 1801, Jefferson was the first United States secretary of state under George Washington and then the nation's second vice president under John Adams.\\n\\nAmong the nation's Founding Fathers, Jefferson is considered unmatched in his intellectual depth and breadth. His passionate writings and advocacy for human rights, including freedom of thought, speech, and religion, were a leading inspiration behind the American Revolution, which ultimately gave rise to the American Revolutionary War, American independence, and the United States Constitution. Jefferson's ideas were globally influential in shaping and inspiring the Age of Enlightenment, which proved transformational in the late 17th and 18th centuries. He was a leading proponent of democracy, republicanism, and individual rights, and produced formative documents and decisions at the state, national, and international levels.\\n\\nDuring the American Revolution, Jefferson represented Virginia in the Second Continental Congress in Philadelphia, which adopted the Declaration of Independence on July 4, 1776. As a Virginia legislator, he drafted a state law for religious freedom. He served as the second Governor of Virginia from 1779 to 1781, during the Revolutionary War. In 1785, Jefferson was appointed the United States Minister to France, and subsequently, the nation's first secretary of state under President George Washington from 1790 to 1793. Jefferson and James Madison organized the Democratic-Republican Party to oppose the Federalist Party during the formation of the First Party System. With Madison, he anonymously wrote the Kentucky and Virginia Resolutions in 1798 and 1799, which sought to strengthen states' rights by nullifying the federal Alien and Sedition Acts.\\n\\nJefferson and Federalist John Adams became friends as well as political rivals, serving in the Continental Congress and drafting the Declaration of Independence together. In the 1796 presidential election between the two, Jefferson came in second, which according to electoral procedure at the time, made him vice president to Adams. Jefferson challenged Adams again in 1800 and won the presidency. After his term in office, Jefferson eventually reconciled with Adams and they shared a correspondence that lasted 14 years. He and Adams both died on the same day, July 4, 1826, which was also the 50th anniversary of Declaration of Independence.\\n\\nAs president, Jefferson pursued the nation's shipping and trade interests against Barbary pirates and aggressive British trade policies. Starting in 1803, he promoted a western expansionist policy with the Louisiana Purchase, which doubled the nation's claimed land area. To make room for settlement, Jefferson began the process of Indian tribal removal from the newly acquired territory. As a result of peace negotiations with France, his administration reduced military forces. He was re-elected in 1804, but his second term was beset with difficulties at home, including the trial of former vice president Aaron Burr. In 1807, American foreign trade was diminished when Jefferson implemented the Embargo Act in response to British threats to U.S. shipping. The same year, Jefferson signed the Act Prohibiting Importation of Slaves.\\n\\nJefferson was a plantation owner, lawyer, and politician, and mastered many disciplines including surveying, mathematics, horticulture, and mechanics. He was also an architect in the Palladian tradition. Jefferson's keen interest in religion and philosophy led to his appointment as president of the American Philosophical Society. He largely shunned organized religion but was influenced by Christianity, Epicureanism, and deism. Jefferson rejected fundamental Christianity, denying Christ's divinity. A philologist, Jefferson knew several languages. He was a prolific letter writer and corresponded with many prominent people, including Edward Carrington, John Taylor of Caroline, and James Madison. In 1785, Jefferson authored Notes on the State of Virginia, considered perhaps the most important American book published before 1800. Jefferson championed the ideals, values, and teachings of the Enlightenment.\\n\\nSince the 1790s, Jefferson was rumored to have had children by his sister-in-law and slave Sally Hemings, leading to what is known as the Jefferson-Hemings controversy. A 1998 DNA test concluded that one of Sally Hemings's children, Eston Hemings, was of the Jefferson male line. According to scholarly consensus, based on documentary and statistical evaluation, as well as oral history, Jefferson probably fathered at least six children with Hemings, including four that survived to adulthood.\\n\\nAfter retiring from public office, Jefferson founded the University of Virginia. Presidential scholars and historians generally praise Jefferson's public achievements, including his advocacy of religious freedom and tolerance in Virginia, his peaceful acquisition of the Louisiana Territory from France without war or controversy, and his ambitious and successful Lewis and Clark Expedition. Some modern historians are critical of Jefferson's personal involvement with slavery. Jefferson is consistently ranked among the top ten presidents of American history. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \""
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load \"/home/li/basu_workspace/nanoGPT/data/dolly/sen_lens.bin\"ArithmeticError\n",
    "sen_lens = np.fromfile(\"/home/li/basu_workspace/nanoGPT/data/dolly/sen_lens.bin\", dtype=np.int16)\n",
    "sen_lens = sen_lens.reshape(-1, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import sys\n",
    "from llamaTokenizer import LLaMAtokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokenizer_path = \"/home/li/basu_workspace/llama/tokenizer.model\"\n",
    "tokenizer = LLaMAtokenizer(model_path=tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15043, 3186, 15043, 17948, 2, 263, 284, 283, 285, 2176, 2] Hello world Hello dance aalou fdf\n"
     ]
    }
   ],
   "source": [
    "enc = lambda s: tokenizer.encode(s, bos=False, eos=True)\n",
    "dec = lambda s: tokenizer.decode(s)\n",
    "e = enc(\"Hello world Hello dance\") + enc(\"aalou fdf\")\n",
    "d = dec(e)\n",
    "print(e, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/li/basu_workspace/nanoGPT/data/databricks-dolly-15k.jsonl') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "data = [json.loads(line) for line in data]\n",
    "data_cleaned  = [\"User: \" + instruct['instruction'] + \"\\nBot: \" + instruct['response'] for instruct in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = []\n",
    "for sentence in data_cleaned:\n",
    "    encoded.append(enc(sentence))\n",
    "assert len (encoded) == len(data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(encoded[i]) for i in range(len(encoded))]\n",
    "\n",
    "sorted_lens = sorted(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl = plt.hist(sorted_lens[:-200], bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(897, 2048)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 2048\n",
    "### create new encoded if len(encoded[i]) < seq_len\n",
    "\n",
    "encoded = [encoded[i] for i in range(len(encoded)) if len(encoded[i]) < seq_len]\n",
    "\n",
    "comb = np.zeros((len(encoded), seq_len), dtype=np.int32)\n",
    "j = 0\n",
    "k = 0\n",
    "sen_lens = []\n",
    "l = []\n",
    "for i in encoded:\n",
    "    if k + len(i) > seq_len:\n",
    "        sen_lens.append(l)\n",
    "        l = [len(i)]\n",
    "        j+=1\n",
    "        k=len(i)\n",
    "    else:\n",
    "        k+=len(i)\n",
    "        l.append(len(i))\n",
    "    comb[j, k-len(i):k] = i\n",
    "    \n",
    "## check if all comb[i] are zero\n",
    "\n",
    "for i in range(len(comb)):\n",
    "    if np.sum(comb[i]) == 0:\n",
    "        num_sen = i-1\n",
    "        break\n",
    "\n",
    "comb = comb[:num_sen]\n",
    "\n",
    "print(comb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(i) for i in sen_lens])\n",
    "## pad sen_lens with zeros\n",
    "sen_lens = [i + [0]*(max_len - len(i)) for i in sen_lens]\n",
    "sen_lens = np.array([np.array(i) for i in sen_lens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 41,  21,  43, ...,   0,   0,   0],\n",
       "       [ 66,  51, 113, ...,   0,   0,   0],\n",
       "       [156, 110,  20, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 64,  35,  56, ...,   0,   0,   0],\n",
       "       [325, 233,  18, ...,   0,   0,   0],\n",
       "       [ 81,  24,  19, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([826, 770, 404, 473])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "ix = torch.randint(comb.shape[0], (batch_size,))\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = comb\n",
    "mask_data = sen_lens\n",
    "ix = torch.randint(data.shape[0], (batch_size,))\n",
    "\n",
    "x = torch.stack([torch.from_numpy((data[i][:-1]).astype(np.int64)) for i in ix])\n",
    "y = torch.stack([torch.from_numpy((data[i+1][1:]).astype(np.int64)) for i in ix])\n",
    "mask = torch.stack([torch.from_numpy(make_mask(mask_data[i])) for i in ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 0), dtype=float64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "21\n",
      "43\n",
      "39\n",
      "36\n",
      "52\n",
      "193\n",
      "26\n",
      "24\n",
      "208\n",
      "1301\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def make_mask(inp):\n",
    "    mask = np.zeros((seq_len, seq_len))\n",
    "    ptr = 0\n",
    "    for len in inp:\n",
    "        if len == 0:\n",
    "            break\n",
    "        mask[ptr:ptr+len, ptr:ptr+len] = np.tril(np.ones((len, len)))\n",
    "        ptr += len\n",
    "    return mask\n",
    "\n",
    "mask = make_mask(sen_lens[0])\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Total     |   Reserved   |   Allocated   |Res - Allocated|      Max      \n",
      "     3.5GB     |     2.5GB     |    1.43GB    |    1.12GB    |    8.55GB    \n"
     ]
    }
   ],
   "source": [
    "max_spaces = 15\n",
    "print_space = lambda x: ((max_spaces - len(x))//2)*\" \" +  x + ((max_spaces - len(x))//2)*\" \"\n",
    "t, r, a, f, max_mem = 3.5, 2.5, 1.4325, 1.12343, 8.55\n",
    "print(\"|\".join([print_space(i) for i in [\"Total\", \"Reserved\", \"Allocated\", \"Res - Allocated\", \"Max\"]]))\n",
    "print(\"|\".join([ print_space(i) for i in [str(round(x, 2)) + 'GB' for x in [t, r, a, f, max_mem]]]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "path = \"/home/li/basu_workspace/nanoGPT/out/shakespeare_finetune_1685411428.4683979/tensor(2.0312)_ckpt.pt\"\n",
    "ckpt = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt['model']['transformer.wte.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 35384 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'llama'\n",
    "\n",
    "model, model_args = load_model(model_type, out_dir, device, learning_block, influence, init_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([1,2,3], device='meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "out_dir = \"/home/li/basu_workspace/nanoGPT/out/shakespeare_finetune_1685411428.4683979/tensor(2.0312)_ckpt.pt\"\n",
    "device = 'cuda'\n",
    "\n",
    "checkpoint = torch.load(out_dir, map_location=device)\n",
    "checkpoint_model_args = checkpoint['model_args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['n_layers', 'n_heads', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "    model_args[k] = checkpoint_model_args[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_layers', 'n_heads', 'learning_block', 'influence', 'vocab_size', 'max_seq_len', 'n_embed'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_model_args.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from utils import load_model\n",
    "from llamaTokenizer import LLaMAtokenizer\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = ['resume', 'llama', 'gpt2-small', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'][1] # or 'resume' or 'gpt2-medium' or 'gpt2-large' or 'gpt2-xl'\n",
    "out_dir = '/home/li/basu_workspace/nanoGPT/harrypotter-learning-block_1684388718.5518227' # ignored if init_from is not 'resume'\n",
    "start = \"User: Capital of France? \\n Bot: Paris \\n User: Capital of India \\n Bot:\"  # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples =  3  # number of samples to draw\n",
    "max_new_tokens = 10 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# learning block\n",
    "learning_block = True\n",
    "influence = 0\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "model_type = 'llama' if 'llama' in init_from else 'gpt2'\n",
    "\n",
    "# sampling = \"continuous\"\n",
    "sampling = \"discrete\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "torch.set_default_dtype(ptdtype)\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# model\n",
    "\n",
    "t = time.time()\n",
    "model, model_args = load_model(model_type, out_dir, device, learning_block,\n",
    "                                influence, init_from, load_state_dict=True)\n",
    "\n",
    "model.eval()\n",
    "# model.to(device)\n",
    "print(model)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "\n",
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "else:\n",
    "\n",
    "    if model_type == 'gpt2':\n",
    "        # ok let's assume gpt-2 encodings by default\n",
    "        print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "        decode = lambda l: enc.decode(l)\n",
    "\n",
    "    elif model_type == 'llama':\n",
    "        tokenizer_path = \"/home/li/basu_workspace/llama/tokenizer.model\"\n",
    "        tokenizer = LLaMAtokenizer(model_path=tokenizer_path)\n",
    "        encode = lambda s: tokenizer.encode(s, bos=True, eos=False)\n",
    "        decode = lambda l: tokenizer.decode(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import safetensors\n",
    "from safetensors.torch import save_model\n",
    "\n",
    "save_model(model, 'my_model.safetensors')\n",
    "\n",
    "# tensors = {}\n",
    "# with safe_open(\"model.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "#    for key in f.keys():\n",
    "#        tensors[key] = f.get_tensor(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import safetensors\n",
    "safetensors.torch.load_model(model, 'my_model.safetensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/li/basu_workspace/nanoGPT/test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m print_gpu_utilization()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# model.half()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# model.to(device)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/basu/lib/python3.8/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/basu/lib/python3.8/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/basu/lib/python3.8/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/basu/lib/python3.8/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "print_gpu_utilization()\n",
    "\n",
    "# model.half()\n",
    "# model.to(device)\n",
    "for p in model.parameters():\n",
    "    print(p.shape, p.dtype, p.device)\n",
    "    break\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 26953.66 MB\n"
     ]
    }
   ],
   "source": [
    "### get size of model\n",
    "size = sum(p.numel() for p in model.parameters()) * 4 / 1e6\n",
    "print(f\"Model size: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating sample 1 of 3\n",
      "User: Capital of France? \n",
      " Bot: Paris \n",
      " User: Capital of India \n",
      " Bot: Delhi \n",
      " User: Capital of USA \n",
      "---------------\n",
      "generating sample 2 of 3\n",
      "User: Capital of France? \n",
      " Bot: Paris \n",
      " User: Capital of India \n",
      " Bot: New Delhi \n",
      " User: Capital of Italy\n",
      "---------------\n",
      "generating sample 3 of 3\n",
      "User: Capital of France? \n",
      " Bot: Paris \n",
      " User: Capital of India \n",
      " Bot: Delhi \n",
      " User: Capital of Iran \n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "if sampling == \"discrete\":\n",
    "    # encode the beginning of the prompt\n",
    "    if start.startswith('FILE:'):\n",
    "        with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "            start = f.read()\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        # with ctx:\n",
    "            for k in range(num_samples):\n",
    "                print(\"generating sample\", k+1, \"of\", num_samples)\n",
    "\n",
    "                start_ids = encode(start)\n",
    "                tkns = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "                y = model.generate(tkns, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "                print(decode(y[0].tolist()))\n",
    "                print('---------------')                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
