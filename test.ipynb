{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.text import Perplexity\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3696, dtype=torch.float64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = Perplexity()\n",
    "\n",
    "logits = torch.tensor([[[0.1, 0.9]]])\n",
    "labels = torch.tensor([[0]])\n",
    "\n",
    "metric.update(logits, labels)\n",
    "\n",
    "metric.compute()\n",
    "logits = torch.tensor([[[0.3, 0.6]]])\n",
    "labels = torch.tensor([[1]])\n",
    "\n",
    "metric.update(logits, labels)\n",
    "metric.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.830029230684137"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "## perplexity = exp(sum -log(p_i) / N)\n",
    "\n",
    "perplexity = np.exp\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations per epoch: 7318\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "data_type = 'instruct'\n",
    "\n",
    "block_size = 2048\n",
    "dataset = 'instruct2'\n",
    "tokens_per_iter = block_size*32\n",
    "data_dir = os.path.join(os.path.dirname(os.path.realpath(\"__file__\")), 'data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "print(\"Iterations per epoch:\", train_data.shape[0] // tokens_per_iter)\n",
    "\n",
    "\n",
    "if data_type == 'instruct':\n",
    "    mask_train = np.memmap(os.path.join(data_dir, 'inp_shape_train.bin'), dtype=np.uint16, mode='r')\n",
    "    mask_val = np.memmap(os.path.join(data_dir, 'inp_shape_val.bin'), dtype=np.uint16, mode='r')\n",
    "    train_data = train_data.reshape(-1, block_size)\n",
    "    val_data = val_data.reshape(-1, block_size)\n",
    "    mask_train = mask_train.reshape(train_data.shape[0], -1)\n",
    "    mask_val = mask_val.reshape(val_data.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/li/basu_workspace/nanoGPT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "pth = (os.path.abspath(os.path.dirname(os.path.dirname(os.path.dirname(\"__file__\")))))\n",
    "\n",
    "print(pth)\n",
    "\n",
    "sys.path.append(pth)\n",
    "from llamaTokenizer import LLaMAtokenizer\n",
    "\n",
    "tokenizer_path = os.path.join(os.path.dirname(pth), \"cptData/lit-llama/tokenizer.model\")\n",
    "\n",
    "train_frac = 0.9\n",
    "seq_len = 2048\n",
    "dataset = 'instruct'\n",
    "\n",
    "tokenizer = LLaMAtokenizer(model_path=tokenizer_path)\n",
    "enc = lambda s: tokenizer.encode(s, bos=False, eos=True)\n",
    "dec = lambda s: tokenizer.decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1176357557357517"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(811240*0.1)/val_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from tqdm import trange\n",
    "\n",
    "pth = os.path.dirname(os.path.dirname(os.path.abspath(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))))\n",
    "\n",
    "###################################################################################################                          \n",
    "\n",
    "## alpaca cleaned\n",
    "with open(os.path.join(pth, 'data/alpaca/alpaca_data_cleaned.json')) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "data_cleaned  = [\"###User: \" + instruct['input'] + \"\\n\" + instruct['instruction'] + \"\\n###Bot: \" + instruct['output'] for instruct in data]\n",
    "\n",
    "print(\"Number of examples in alpaca: \", len(data_cleaned))\n",
    "\n",
    "## dolly\n",
    "with open(os.path.join(pth, 'data/dolly/databricks-dolly-15k.jsonl')) as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "data = [json.loads(line) for line in data]\n",
    "data_cleaned_dolly  = [\"###User: \" + instruct['context'] + \"\\n\" +  instruct['instruction'] + \"\\n###Bot: \" + instruct['response'] for instruct in data]\n",
    "\n",
    "print(\"Number of examples in dolly: \", len(data_cleaned_dolly))\n",
    "\n",
    "data_cleaned += data_cleaned_dolly\n",
    "############################################################################################################\n",
    "\n",
    "sys.path.append(pth)\n",
    "from llamaTokenizer import LLaMAtokenizer\n",
    "\n",
    "tokenizer_path = os.path.join(os.path.dirname(pth), \"cptData/lit-llama/tokenizer.model\")\n",
    "\n",
    "train_frac = 0.9\n",
    "seq_len = 2048\n",
    "dataset = 'instruct'\n",
    "\n",
    "tokenizer = LLaMAtokenizer(model_path=tokenizer_path)\n",
    "enc = lambda s: tokenizer.encode(s, bos=False, eos=True)\n",
    "dec = lambda s: tokenizer.decode(s)\n",
    "\n",
    "encoded = [enc(data_cleaned[i]) for i in trange(len(data_cleaned))]\n",
    "assert len (encoded) == len(data_cleaned)\n",
    "\n",
    "encoded = [encoded[i] for i in range(len(encoded)) if len(encoded[i]) < seq_len]\n",
    "\n",
    "comb = np.ones((len(encoded), seq_len), dtype=np.int32)*2 ## pad with eos_token_id\n",
    "j, k = 0, 0\n",
    "sen_lens,l = [], []\n",
    "\n",
    "for i in encoded:\n",
    "    if k + len(i) > seq_len:\n",
    "        sen_lens.append(l)\n",
    "        l = [len(i)]\n",
    "        j+=1\n",
    "        k=len(i)\n",
    "    else:\n",
    "        k+=len(i)\n",
    "        l.append(len(i))\n",
    "    comb[j, k-len(i):k] = i\n",
    "\n",
    "for i in range(len(comb)):\n",
    "    if np.all(comb[i] == 2):\n",
    "        num_sen = i-1\n",
    "        break\n",
    "\n",
    "comb = comb[:num_sen]\n",
    "\n",
    "max_len = max([len(i) for i in sen_lens])\n",
    "\n",
    "sen_lens = [i + [0]*(max_len - len(i)) for i in sen_lens]\n",
    "\n",
    "inp_shape_train = sen_lens[:int(train_frac*len(comb))]\n",
    "inp_shape_val = sen_lens[int(train_frac*len(comb)):]\n",
    "\n",
    "train_ids = comb[:int(train_frac*len(comb))]\n",
    "val_ids = comb[int(train_frac*len(comb)):]\n",
    "\n",
    "assert train_ids.shape[0] == len(inp_shape_train)\n",
    "assert val_ids.shape[0] == len(inp_shape_val)\n",
    "\n",
    "print(\"train_ids.shape: \", train_ids.shape)\n",
    "print(\"val_ids.shape: \", val_ids.shape)\n",
    "\n",
    "inp_shape_train = [item for sublist in inp_shape_train for item in sublist]\n",
    "inp_shape_val = [item for sublist in inp_shape_val for item in sublist]\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "inp_shape_train = np.array(inp_shape_train, dtype=np.uint16)\n",
    "inp_shape_val = np.array(inp_shape_val, dtype=np.uint16)\n",
    "\n",
    "train_ids.tofile(os.path.join(pth, 'data/{dataset}/train.bin'.format(dataset=dataset)))\n",
    "val_ids.tofile(os.path.join(pth, 'data/{dataset}/val.bin'.format(dataset=dataset)))\n",
    "inp_shape_train.tofile(os.path.join(pth, 'data/{dataset}/inp_shape_train.bin'.format(dataset=dataset)))\n",
    "inp_shape_val.tofile(os.path.join(pth, 'data/{dataset}/inp_shape_val.bin'.format(dataset=dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Learning Block True\n",
      "Iterations per epoch: 25\n",
      "Creating and loading model\n",
      "Initializing from OG weights: /home/li/basu_workspace/nanoGPT/../cptData/lit-llama/7B/lit-llama.pth\n",
      "Creating model 111.51918911933899\n",
      "GPU memory used: 0.0\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB |  0.0GB  |  0.0GB  |  0.0GB  \n",
      "Loading state dict 18.009955883026123\n",
      "GPU memory used: 0.0\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB |  0.0GB  |  0.0GB  |  0.0GB  \n",
      "Total time to load model:  129.5314872264862\n",
      "model to GPU 11.540282011032104\n",
      "GPU memory used: 13.6259765625\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 12.81GB |  12.8GB  |  12.8GB  \n",
      "setting requires_grad=False for all layers except learning block\n",
      "total params with requires grad 134.365184 M\n",
      "num decayed parameter tensors: 64, with 134,217,728 parameters\n",
      "num non-decayed parameter tensors: 64, with 147,456 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from utils import load_model, Sampler, get_batch, configure_optimizers, time_gpu, get_pred_idxs\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "sample_start = \"The king exclaimed thou\"\n",
    "max_new_tokens=100\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # or 'resume' or 'gpt2-medium' or 'gpt2-large' or 'gpt2-xl' or 'eval_llama' or 'llama'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = \"transformers\"\n",
    "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'openwebtext'\n",
    "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
    "batch_size = 32 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 2048\n",
    "# model\n",
    "n_layers = 12\n",
    "n_heads = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "torch.set_default_dtype(ptdtype)\n",
    "\n",
    "# learning block\n",
    "learning_block = False\n",
    "influence = 0.5\n",
    "\n",
    "## instruct\n",
    "data_type = None\n",
    "break_at_eos=False\n",
    "eos_token_id=1\n",
    "train_on_user_only = False\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "\n",
    "import time\n",
    "\n",
    "eval_interval = 5\n",
    "eval_iters = 40\n",
    "wandb_log = True # feel free to turn on\n",
    "wandb_project = 'learning-block'\n",
    "\n",
    "sample_start = \"User: Write a few words on Einstein.\\nBot:\"\n",
    "max_new_tokens = 100\n",
    "\n",
    "wandb_run_name = 'lb2_llama_dolly' + '_' + time.strftime(\"%m%d-%H%M\") ## train_type,  model , dataset\n",
    "dataset = 'dolly'\n",
    "init_from = 'llama'\n",
    "\n",
    "data_type = 'instruct'\n",
    "out_dir = '../cptData/out/' + wandb_run_name \n",
    "\n",
    "# only save checkpoints if the validation loss improves\n",
    "always_save_checkpoint = False\n",
    "\n",
    "# the number of examples per iter:\n",
    "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
    "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
    "batch_size = 1\n",
    "gradient_accumulation_steps = 32\n",
    "max_iters = 100\n",
    "\n",
    "learning_block = True\n",
    "\n",
    "learning_rate = 3e-4\n",
    "lr_decay_iters = max_iters\n",
    "decay_lr = True\n",
    "warmup_iters = max_iters // 10\n",
    "\n",
    "compile = False\n",
    "\n",
    "break_at_eos = False\n",
    "eos_token_id = 2\n",
    "\n",
    "train_on_user_only = True\n",
    "\n",
    "\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "model_type = 'llama' if 'llama' in init_from else 'gpt2'\n",
    "\n",
    "## if torch version < 2 set compile to False\n",
    "if torch.__version__[0] == '1' and compile:\n",
    "    print(\"PyTorch version < 2.0, disabling compilation\")\n",
    "    compile = False\n",
    "\n",
    "print(\"Using Learning Block\", learning_block)\n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank # each process gets a different seed\n",
    "    assert gradient_accumulation_steps % torch.cuda.device_count() == 0\n",
    "    gradient_accumulation_steps //= torch.cuda.device_count()\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader\n",
    "data_dir = os.path.join(os.path.dirname(os.path.realpath(\"__file__\")), 'data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "print(\"Iterations per epoch:\", train_data.shape[0] // tokens_per_iter)\n",
    "\n",
    "\n",
    "if dataset == 'dolly':\n",
    "    mask_train = np.memmap(os.path.join(data_dir, 'inp_shape_train.bin'), dtype=np.uint16, mode='r')\n",
    "    mask_val = np.memmap(os.path.join(data_dir, 'inp_shape_val.bin'), dtype=np.uint16, mode='r')\n",
    "    train_data = train_data.reshape(-1, block_size)\n",
    "    val_data = val_data.reshape(-1, block_size)\n",
    "    mask_train = mask_train.reshape(train_data.shape[0], -1)\n",
    "    mask_val = mask_val.reshape(val_data.shape[0], -1)\n",
    "\n",
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layers=n_layers, n_heads=n_heads, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout, learning_block=learning_block) # start with model_args from command line\n",
    "\n",
    "\n",
    "model, model_args = load_model(model_type, out_dir, device, learning_block, influence, init_from)\n",
    "\n",
    "with time_gpu(device, 'model to GPU'):\n",
    "    model.to(device)\n",
    "\n",
    "## set requires grad to false for all layers except learning block\n",
    "\n",
    "if learning_block:\n",
    "    print(\"setting requires_grad=False for all layers except learning block\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if \"learning_block\" not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "## total number of parameters that requires grad\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"total params with requires grad\", total_params/1e6, \"M\")\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "# if init_from == 'resume':\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# compile the model\n",
    "if compile:\n",
    "    with time_gpu(device, 'compiling model'):\n",
    "        print(\"compiling the model... (takes a ~minute)\")\n",
    "        model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n",
    "# wrap model into DDP container\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "sampler = Sampler(model_name = model_type, start = sample_start, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_log  = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 868, 32000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(pred_idxs, device=device).unsqueeze(2).repeat(1,1,logits.size(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -5.9688, -12.4375,   4.2812,  ...,  -3.2031,  -2.6562,  -4.0000],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0][26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -5.9688, -12.4375,   4.2812,  ...,  -3.2031,  -2.6562,  -4.0000],\n",
       "         [ -5.8125,  -7.8750,   4.8125,  ...,  -3.8125,  -1.5312,  -3.0781],\n",
       "         [ -2.5781,  -5.9375,   9.7500,  ...,  -1.2031,  -1.2422,  -1.2969],\n",
       "         ...,\n",
       "         [ -1.8750,  -2.7812,  14.8125,  ...,   0.9297,  -1.5312,  -3.6719],\n",
       "         [ -3.4375, -18.5000,   4.0938,  ...,   2.5469,  -0.0437,   0.8789],\n",
       "         [ -3.7969,  -5.3750,   8.6875,  ...,   0.2402,   0.4258,  -1.3047]]],\n",
       "       device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.gather(1, torch.tensor(pred_idxs, device=device).unsqueeze(2).repeat(1,1,logits.size(-1))).squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -9.3125,  -9.3125,  -9.3125,  ...,  -9.3125,  -9.3125,  -9.3125],\n",
       "         [-14.3125, -14.3125, -14.3125,  ..., -14.3125, -14.3125, -14.3125],\n",
       "         [ -2.4531,  -2.4531,  -2.4531,  ...,  -2.4531,  -2.4531,  -2.4531],\n",
       "         ...,\n",
       "         [  2.6719,   2.6719,   2.6719,  ...,   2.6719,   2.6719,   2.6719],\n",
       "         [  5.3438,   5.3438,   5.3438,  ...,   5.3438,   5.3438,   5.3438],\n",
       "         [  4.3438,   4.3438,   4.3438,  ...,   4.3438,   4.3438,   4.3438]]],\n",
       "       device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Training Step 23.44026803970337\n",
      "GPU memory used: 1.333984375\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 50.04GB | 31.13GB |  48.2GB  \n",
      "iter 4: loss 1.8281, time 23531.59ms\n",
      "Sampling from trained model\n",
      "User: Write a few words on Einstein.\n",
      "Bot: Shauna Sampath has been at the forefront of social entrepreneurism for the last decade. The founder of Latticework, Shauna Sampath is on a mission to build a 21st century infrastructure that supports the poor and disadvantaged. The Shauna Sampath organization is also set up to carry this mission. As one of the 2005 100 most influential people in the world that\n",
      "---------------\n",
      "User: Write a few words on Einstein.\n",
      "Bot: 3rd August and the 4th of July 1776 was the day of the Boston Tea party.\n",
      "It was an incident that marked the start of the American Revolution.\n",
      "Labels: 1776, boston tea party, december, december, First Day of Christmas, King George, mother of George, second day of christmas, The Boston Tea Party\n",
      "Susannah in 1696 is credited for creating the\n",
      "---------------\n",
      "Ealuate 44.457242250442505\n",
      "GPU memory used: 0.91015625\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 50.95GB | 31.13GB |  48.2GB  \n",
      "step 5: train loss 1.9375, val loss 1.9531\n",
      "saving checkpoint to ../cptData/out/lb2_llama_dolly_0606-0405\n",
      "Training Step 23.304295539855957\n",
      "GPU memory used: 0.0\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 50.95GB | 31.09GB |  48.2GB  \n",
      "iter 5: loss 1.9922, time 97913.09ms\n",
      "Training Step 22.783890962600708\n",
      "GPU memory used: 0.0\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 50.95GB | 31.11GB | 48.24GB \n",
      "iter 6: loss 1.6484, time 22785.13ms\n",
      "Training Step 7.750816822052002\n",
      "GPU memory used: 0.0\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 50.95GB | 47.96GB | 48.24GB \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 116\u001b[0m\n\u001b[1;32m    113\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m gradient_accumulation_steps \u001b[39m# scale the loss to account for gradient accumulation\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[39m# immediately async prefetch next batch while model is doing the forward pass on the GPU\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     X, Y, mask, pred_idxs \u001b[39m=\u001b[39m get_batch(\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, block_size, batch_size, device_type, device, train_data, val_data,\n\u001b[1;32m    117\u001b[0m                            data_type \u001b[39m=\u001b[39;49m data_type, mask_train \u001b[39m=\u001b[39;49m mask_train, mask_val \u001b[39m=\u001b[39;49m mask_val, train_on_user_only \u001b[39m=\u001b[39;49m train_on_user_only)\n\u001b[1;32m    119\u001b[0m     scaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    120\u001b[0m \u001b[39m# clip the gradient\u001b[39;00m\n",
      "File \u001b[0;32m~/basu_workspace/nanoGPT/utils.py:155\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(split, block_size, batch_size, device_type, device, train_data, val_data, data_type, mask_train, mask_val, train_on_user_only)\u001b[0m\n\u001b[1;32m    153\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([torch\u001b[39m.\u001b[39mfrom_numpy((data[i][:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint64)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m ix])\n\u001b[1;32m    154\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([torch\u001b[39m.\u001b[39mfrom_numpy((data[i][\u001b[39m1\u001b[39m:])\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint64)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m ix])\n\u001b[0;32m--> 155\u001b[0m mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack([torch\u001b[39m.\u001b[39;49mfrom_numpy(make_mask(mask_data[i], block_size)[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m ix])\n\u001b[1;32m    156\u001b[0m \u001b[39m## conver mask to bool\u001b[39;00m\n\u001b[1;32m    157\u001b[0m mask \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39mbool()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Sampling from trained model\")\n",
    "    sampler.generate(model, max_new_tokens=max_new_tokens, break_at_eos = break_at_eos,eos_token_id = eos_token_id) \n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y, mask, pred_idxs = get_batch(split, block_size, batch_size, device_type, device, train_data, val_data, \n",
    "                                   data_type = data_type, mask_train = mask_train, mask_val = mask_val, train_on_user_only = train_on_user_only)\n",
    "                \n",
    "            with ctx:\n",
    "                logits = model(X, mask = mask)\n",
    "                            \n",
    "            if train_on_user_only:\n",
    "                logits = logits.gather(1, torch.tensor(pred_idxs, device=device).unsqueeze(2).repeat(1,1,logits.size(-1))).squeeze(2)\n",
    "                Y = Y.gather(1, torch.tensor(pred_idxs, device=device)).squeeze(1)\n",
    "                \n",
    "                # print(logits.shape, Y.shape)\n",
    "\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=-1)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# logging\n",
    "if wandb_log and master_process:\n",
    "    import wandb\n",
    "    wandb_api_key = \"84742742b66deb0de22b5dfec52ec1f23a539d9b\"\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, entity='basujindal123',config=config)\n",
    "\n",
    "# training loop\n",
    "X, Y, mask, pred_idxs = get_batch('train', block_size, batch_size, device_type, device, train_data, val_data,\n",
    "                       data_type = data_type, mask_train = mask_train, mask_val = mask_val, train_on_user_only = train_on_user_only)\n",
    "\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
    "iter_num_resume = iter_num\n",
    "\n",
    "print(\"Training\")\n",
    "for iter_num in range(iter_num_resume, max_iters+1):\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        \n",
    "        with time_gpu(device,'Ealuate'):\n",
    "            losses = estimate_loss()\n",
    "            \n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "            })\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    # 'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir,'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    with time_gpu(device, 'Training Step'):\n",
    "        \n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            if ddp:\n",
    "                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "            with ctx:\n",
    "                logits = model(X, mask = mask)\n",
    "                \n",
    "            if train_on_user_only:\n",
    "                lgts = logits.gather(1, torch.tensor(pred_idxs, device=device).unsqueeze(2).repeat(1,1,logits.size(-1))).squeeze(2)\n",
    "                Y = Y.gather(1, torch.tensor(pred_idxs, device=device)).squeeze(1)\n",
    "                \n",
    "                # print(lgts.shape, Y.shape)\n",
    "\n",
    "            loss = F.cross_entropy(lgts.view(-1, lgts.size(-1)), Y.view(-1), ignore_index=-1)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "                \n",
    "            # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "            X, Y, mask, pred_idxs = get_batch('train', block_size, batch_size, device_type, device, train_data, val_data,\n",
    "                                   data_type = data_type, mask_train = mask_train, mask_val = mask_val, train_on_user_only = train_on_user_only)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "        # clip the gradient\n",
    "        if grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a senatence to continue:\n",
      "['User: Hi.\\nBot:']\n",
      "Enter a senatence to continue:\n",
      "['User: .\\nBot:']\n",
      "Enter a senatence to continue:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[39m## take input\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnter a senatence to continue:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     start \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39minput\u001b[39;49m())\n\u001b[1;32m      5\u001b[0m     start \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUser: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m start \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mBot:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m([start])\n",
      "File \u001b[0;32m~/anaconda/envs/basu/lib/python3.9/site-packages/ipykernel/kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1191\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[1;32m   1192\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[1;32m   1193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1194\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1195\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1196\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda/envs/basu/lib/python3.9/site-packages/ipykernel/kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ## take input\n",
    "    print(\"Enter a senatence to continue:\")\n",
    "    start = str(input())\n",
    "    start = \"###User: \" + start + \".\\n###Bot:\"\n",
    "    print([start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average original version duration: 1.5621185302734374e-05\n",
      "Average optimized version duration: 8.759498596191407e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "# Generate random integer data\n",
    "data = [random.randint(0, 10) for _ in range(100)]\n",
    "\n",
    "# Randomly select sep and sen\n",
    "sep = random.sample(data, 5)\n",
    "sen = random.sample(data, 20)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    results=[]\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            results.append((ind,ind+sll-1))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Original get_pred_idxs function\n",
    "def get_pred_idxs(sen):\n",
    "    sep = encode(\"\\nBot: \")[1:-2]\n",
    "    idxs = find_sub_list(sep, sen)\n",
    "\n",
    "    pred_idxs = []\n",
    "    for i in range(len(idxs)):\n",
    "        for j in range(idxs[i][1], len(sen)):\n",
    "            if val_data[0][j] == 2:\n",
    "                pred_idxs += [sen[p] for p in range(idxs[i][1] +1, j+1)]\n",
    "                break\n",
    "\n",
    "    return pred_idxs\n",
    "\n",
    "# Optimized get_pred_idxs function\n",
    "def get_pred_idxs_optimized(sen, sep):\n",
    "    idxs = find_sub_list(sep, sen)\n",
    "\n",
    "    pred_idxs = []\n",
    "    val_data_0 = val_data[0]  # Assuming val_data is defined outside the function\n",
    "    for start, end in idxs:\n",
    "        for j in range(start + 1, len(sen)):\n",
    "            try:\n",
    "                idx = val_data_0.index(2, j)\n",
    "                pred_idxs.extend(sen[p] for p in range(start + 1, idx + 1))\n",
    "                break\n",
    "            except ValueError:\n",
    "                break\n",
    "\n",
    "    return pred_idxs\n",
    "\n",
    "# Run the functions multiple times and calculate average duration\n",
    "num_iterations = 50  # Number of iterations to run the functions\n",
    "\n",
    "# Timing the original version\n",
    "original_durations = []\n",
    "for _ in range(num_iterations):\n",
    "    start_time = time.time()\n",
    "    original_result = get_pred_idxs(sen)\n",
    "    original_durations.append(time.time() - start_time)\n",
    "\n",
    "average_original_duration = sum(original_durations) / num_iterations\n",
    "\n",
    "# Timing the optimized version\n",
    "optimized_durations = []\n",
    "for _ in range(num_iterations):\n",
    "    start_time = time.time()\n",
    "    optimized_result = get_pred_idxs_optimized(sen, sep)\n",
    "    optimized_durations.append(time.time() - start_time)\n",
    "\n",
    "average_optimized_duration = sum(optimized_durations) / num_iterations\n",
    "\n",
    "print(\"Average original version duration:\", average_original_duration)\n",
    "print(\"Average optimized version duration:\", average_optimized_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Learning Block True\n",
      "Iterations per epoch: 25\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from utils import load_model, Sampler, get_batch, configure_optimizers, time_gpu, get_pred_idxs\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "sample_start = \"The king exclaimed thou\"\n",
    "max_new_tokens=100\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # or 'resume' or 'gpt2-medium' or 'gpt2-large' or 'gpt2-xl' or 'eval_llama' or 'llama'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = \"transformers\"\n",
    "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'openwebtext'\n",
    "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
    "batch_size = 32 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 2048\n",
    "# model\n",
    "n_layers = 12\n",
    "n_heads = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "torch.set_default_dtype(ptdtype)\n",
    "\n",
    "# learning block\n",
    "learning_block = False\n",
    "influence = 0.5\n",
    "\n",
    "## instruct\n",
    "data_type = None\n",
    "break_at_eos=False\n",
    "eos_token_id=1\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "import time\n",
    "\n",
    "eval_interval = 5\n",
    "eval_iters = 40\n",
    "wandb_log = True # feel free to turn on\n",
    "wandb_project = 'learning-block'\n",
    "\n",
    "sample_start = \"User: Write a few words on Einstein.\\nBot:\"\n",
    "max_new_tokens = 100\n",
    "\n",
    "wandb_run_name = 'lb2_llama_dolly' + '_' + time.strftime(\"%m%d-%H%M\") ## train_type,  model , dataset\n",
    "dataset = 'dolly'\n",
    "init_from = 'llama'\n",
    "\n",
    "data_type = 'instruct'\n",
    "out_dir = '../cptData/out/' + wandb_run_name \n",
    "\n",
    "# only save checkpoints if the validation loss improves\n",
    "always_save_checkpoint = False\n",
    "\n",
    "# the number of examples per iter:\n",
    "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
    "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
    "batch_size = 1\n",
    "gradient_accumulation_steps = 32\n",
    "max_iters = 500\n",
    "\n",
    "learning_block = True\n",
    "\n",
    "learning_rate = 3e-4\n",
    "lr_decay_iters = max_iters\n",
    "decay_lr = True\n",
    "warmup_iters = max_iters // 10\n",
    "\n",
    "compile = False\n",
    "\n",
    "break_at_eos = True\n",
    "eos_token_id = 2\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "model_type = 'llama' if 'llama' in init_from else 'gpt2'\n",
    "\n",
    "## if torch version < 2 set compile to False\n",
    "if torch.__version__[0] == '1' and compile:\n",
    "    print(\"PyTorch version < 2.0, disabling compilation\")\n",
    "    compile = False\n",
    "\n",
    "print(\"Using Learning Block\", learning_block)\n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank # each process gets a different seed\n",
    "    assert gradient_accumulation_steps % torch.cuda.device_count() == 0\n",
    "    gradient_accumulation_steps //= torch.cuda.device_count()\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader\n",
    "data_dir = os.path.join(os.path.dirname(os.path.realpath(\"__file__\")), 'data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "print(\"Iterations per epoch:\", train_data.shape[0] // tokens_per_iter)\n",
    "\n",
    "\n",
    "if dataset == 'dolly':\n",
    "    mask_train = np.memmap(os.path.join(data_dir, 'inp_shape_train.bin'), dtype=np.uint16, mode='r')\n",
    "    mask_val = np.memmap(os.path.join(data_dir, 'inp_shape_val.bin'), dtype=np.uint16, mode='r')\n",
    "    train_data = train_data.reshape(-1, block_size)\n",
    "    val_data = val_data.reshape(-1, block_size)\n",
    "    mask_train = mask_train.reshape(train_data.shape[0], -1)\n",
    "    mask_val = mask_val.reshape(val_data.shape[0], -1)\n",
    "\n",
    "train_on_user_only = True\n",
    "# training loop\n",
    "X, Y, mask, pred_idxs = get_batch('train', block_size, batch_size, device_type, device, train_data, val_data,\n",
    "                       data_type = data_type, mask_train = mask_train, mask_val = mask_val, train_on_user_only = train_on_user_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User: What is the difference between a coastal cruising and blue water sailboat?\\nBot: The designs, accommodations and compromises for these boats can be very different and reflect the likely sea conditions these boats would encounter, expected time at sea and different experiences of passenger comfort.  The primary difference between these boats begins with the shape of the hull since many other design choices tend to be dictated by hull shape.\\n\\nCoastal cruisers, which are generally sailed near shore and in safer weather conditions, \\ntend to have a flatter hull shapes since these boats, which means they tend to \"slap\" the water when coming off a wave.  With a flatter hull shape, boat architects can make greater use of the cabin space and so coastal cruisers will generally have a more spacious interior.  The helm (or the outdoor area from which the boat is navigated) is generally less protected and generally speaking, offers greater comfort and ease of moving around.  Safety, protection and expected time away from marinas mean these boats can refill fresh water, recharge batteries, empty waste tanks more frequently and therefore, tend to have lower capacities.\\n\\nBlue water boats, because they are intended to sail in open waters (hence the moniker \"blue water\") in more unpredictable weather, generally have tapered (or tear drop shaped) hull shapes with heavier keels (generally, lead) and deeper drafts.  With these designs, blue water boats can withstand heavier seas and will \"cut through\" waves rather than \"slap.\"  Generally speaking, blue water boats offer a much more comfortable experience in rougher seas and will have bigger fresh water and fuel tanks and will likely feature generators and other equipment that allows these boats to away for longer periods.\\n\\nBlue water boats feature tapered hulls so the beam or width of the boat narrows the closer it gets to the water line (and below), leaving less room in the interior cabin than an equally sized coastal cruiser.  Foot for foot, blue water boats will feature fewer berths and will have more cramped quarters because of the narrowing beam.\\n\\nBecause blue water boats are intended to sail longer distances and will carry more fuel and fresh water, these boats interior space is further restricted to accomadate these practical requirements.\\n\\nThe differences in comfort, stability and likelihood of capsizing are all captured in mathematical calculations that can provide an abstracted comparison between these boat types so prospective buyers can anticipate the the experiences and trade offs each hull type would provide. User: Given this paragraph about the Roman temple known as the Pantheon, tell me whether we definitively know why the temple was called the Pantheon.\\nBot: We do not know why the temple was called the Pantheon. Even Cassius Dio, a Roman senator who would have had more context by nature of living and ruling in Roman society, was only able to speculate where the name came from. User: How to find the area of a rectangle?\\nBot: A rectangle is a closed two dimensional quadrilateral structure with four right angles (90 degrees).  Opposite sides of a rectangle are parallel and of equal length. Usually, the length of longer sides is called length of a rectangle and the length of shorter sides is called width/breadth of a rectangle.\\nArea of the rectangle is calculated by multiplying the length and width of the rectangle. For example, If the length of the rectangle is l and the width of the rectangle is w then area of the rectangle = (l x w).\\nIf all sides of a rectangle are of equal length then it is called a square. Thus, square is a special case of a rectangle. User: Give me a list of the Adventure motorbikes that I can buy on my LAMS (Learner Approved Motorcycle Scheme) restriction, living in Australia\\nBot: This is not a comprehensive list but you can use this as a starting point:\\nKawasaki Versys-X 300\\nKawazaki KLR650\\nHonda CRF300 Rally\\nBMW G 310 GS\\nKTM 390 Adventure\\nHonda CB500XA\\nYamaha XT250\\nYamaha WR250R\\nSuzuki V-Strom 650XT\\nRoyal Enfield Himalayan\\nSuzuki DRZ400/DR650 User: Which is a species of fish? Turbot or Turbo\\nBot: Turbot User: From the passage identify the names of Zeus’s Roman counterpart. Display the results in a comma separated format.\\nBot: Jupiter, Jove User: Give me a list of the best movies that are sequels\\nBot: Cheaper by the Dozen 2, Night at the Museum 2, Grown-Ups 2, Toy Story 2 User: What is Venture Capital?\\nBot: Venture capital firms invest in early-stage companies in exchange for equity. User: What are the different air routes for traveling between Genova (GOA) and San Francisco (SFO) from fastest to longest?\\nBot: 1 - GOA-MUC-SFO (United Airlines)\\n2 - GOA-FRA-SFO (United Airlines)\\n3 - GOA-AMS-SFO (KLM)\\n4 - GOA-FCO-SFO (ITA Airways)\\n5 - GOA-FCO-JFK-SFO (ITA Airways + Delta Airlines)\\n6 - GOA-FCO-LAX-SFO (ITA Airways + Delta Airlines)\\n7 - GOA-AMS-JFK-SFO (KLM + Delta Airlines)\\n8 - GOA-AMS-ATL-SFO (KLM + Delta Airlines)\\n9 - GOA-AMS-MSP-SFO (KLM + Delta Airlines) User: I do not trust you\\nBot: Oopss..i am trying my best to understand more details and trying to be more accurate day by day but I do understand that it may take some time to build trust with a machine learning model like myself. I can tell you that I do not have personal biases or agendas, and my response here is purely based on the information and context provided to me. \\nI would love to see the statement to changing to \"I trust you\" :) User: What nationality was Morse inventor of the famous code\\nBot: American User: Give me a bulleted list of different ways to get around a city without a car\\nBot: The following are different ways to get around a city without a car:\\n1. Walk\\n2. Take public transportation such as the bus or subway\\n3. Ride a bike\\n4. Use Uber\\n5. Ride a scooter\\n6. Ask a friend with a car\\n7. Hitchhike User: Where did Father\\'s Day originate?\\nBot: Father\\'s Day was founded in Spokane, Washington. User: List the ten episode titles from the fourth season of \"Game of Thrones\" in broadcast order.\\nBot: Two Swords, The Lion and the Rose, Breaker of Chains, Oathkeeper, First of His Name, The Laws of Gods and Men, Mockingbird, The Mountain and the Viper, The Watchers on the Wall, The Children User: Which of these items will be useful to pack when I’m getting ready for soccer practice: cleats, grass, guitar, water bottle, shin guards, helmet.\\nBot: Cleats, shin guards, and a water bottle may be helpful things to pack when you’re getting ready for soccer practice. A helmet is not typically used in soccer, a guitar is a musical instrument, and grass is the material soccer is usually played on but not something you’ll need to pack. User: Is Leonidas from Sparta a real man? What was he famous for?\\nBot: Yes, Leonidas from Sparta was a real man. He was famous for his courage and leadership during the Battle of Thermopylae in 480 BC. User: I need some ideas for a first date.\\nBot: -  Dinner at your favorite pizzeria \\n- A walk around a local park\\n- Mid-afternoon coffee at a local coffee shop\\n- Tour a museum User: Who wrote Gone with the Wind\\nBot: Margaret Mitchell User: From the passage provided, extract the sobriquets Tagore was known by. Separate them with a comma.\\nBot: Gurudeb, Kobiguru, Biswokobi. User: Given a reference text about the Northern Colorado Bears, tell me the different conferences their teams compete in.\\nBot: The Northern Colorado Bears teams compete mostly in the Big Sky Conference. However, some teams compete in the Summit League, Western Athletic Conference, and the Big 12 Conference.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(X.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The designs, accommodations and compromises for these boats can be very different and reflect the likely sea conditions these boats would encounter, expected time at sea and different experiences of passenger comfort.  The primary difference between these boats begins with the shape of the hull since many other design choices tend to be dictated by hull shape.\\n\\nCoastal cruisers, which are generally sailed near shore and in safer weather conditions, \\ntend to have a flatter hull shapes since these boats, which means they tend to \"slap\" the water when coming off a wave.  With a flatter hull shape, boat architects can make greater use of the cabin space and so coastal cruisers will generally have a more spacious interior.  The helm (or the outdoor area from which the boat is navigated) is generally less protected and generally speaking, offers greater comfort and ease of moving around.  Safety, protection and expected time away from marinas mean these boats can refill fresh water, recharge batteries, empty waste tanks more frequently and therefore, tend to have lower capacities.\\n\\nBlue water boats, because they are intended to sail in open waters (hence the moniker \"blue water\") in more unpredictable weather, generally have tapered (or tear drop shaped) hull shapes with heavier keels (generally, lead) and deeper drafts.  With these designs, blue water boats can withstand heavier seas and will \"cut through\" waves rather than \"slap.\"  Generally speaking, blue water boats offer a much more comfortable experience in rougher seas and will have bigger fresh water and fuel tanks and will likely feature generators and other equipment that allows these boats to away for longer periods.\\n\\nBlue water boats feature tapered hulls so the beam or width of the boat narrows the closer it gets to the water line (and below), leaving less room in the interior cabin than an equally sized coastal cruiser.  Foot for foot, blue water boats will feature fewer berths and will have more cramped quarters because of the narrowing beam.\\n\\nBecause blue water boats are intended to sail longer distances and will carry more fuel and fresh water, these boats interior space is further restricted to accomadate these practical requirements.\\n\\nThe differences in comfort, stability and likelihood of capsizing are all captured in mathematical calculations that can provide an abstracted comparison between these boat types so prospective buyers can anticipate the the experiences and trade offs each hull type would provide. We do not know why the temple was called the Pantheon. Even Cassius Dio, a Roman senator who would have had more context by nature of living and ruling in Roman society, was only able to speculate where the name came from. A rectangle is a closed two dimensional quadrilateral structure with four right angles (90 degrees).  Opposite sides of a rectangle are parallel and of equal length. Usually, the length of longer sides is called length of a rectangle and the length of shorter sides is called width/breadth of a rectangle.\\nArea of the rectangle is calculated by multiplying the length and width of the rectangle. For example, If the length of the rectangle is l and the width of the rectangle is w then area of the rectangle = (l x w).\\nIf all sides of a rectangle are of equal length then it is called a square. Thus, square is a special case of a rectangle. This is not a comprehensive list but you can use this as a starting point:\\nKawasaki Versys-X 300\\nKawazaki KLR650\\nHonda CRF300 Rally\\nBMW G 310 GS\\nKTM 390 Adventure\\nHonda CB500XA\\nYamaha XT250\\nYamaha WR250R\\nSuzuki V-Strom 650XT\\nRoyal Enfield Himalayan\\nSuzuki DRZ400/DR650 Turbot Jupiter, Jove Cheaper by the Dozen 2, Night at the Museum 2, Grown-Ups 2, Toy Story 2 Venture capital firms invest in early-stage companies in exchange for equity. 1 - GOA-MUC-SFO (United Airlines)\\n2 - GOA-FRA-SFO (United Airlines)\\n3 - GOA-AMS-SFO (KLM)\\n4 - GOA-FCO-SFO (ITA Airways)\\n5 - GOA-FCO-JFK-SFO (ITA Airways + Delta Airlines)\\n6 - GOA-FCO-LAX-SFO (ITA Airways + Delta Airlines)\\n7 - GOA-AMS-JFK-SFO (KLM + Delta Airlines)\\n8 - GOA-AMS-ATL-SFO (KLM + Delta Airlines)\\n9 - GOA-AMS-MSP-SFO (KLM + Delta Airlines) Oopss..i am trying my best to understand more details and trying to be more accurate day by day but I do understand that it may take some time to build trust with a machine learning model like myself. I can tell you that I do not have personal biases or agendas, and my response here is purely based on the information and context provided to me. \\nI would love to see the statement to changing to \"I trust you\" :) American The following are different ways to get around a city without a car:\\n1. Walk\\n2. Take public transportation such as the bus or subway\\n3. Ride a bike\\n4. Use Uber\\n5. Ride a scooter\\n6. Ask a friend with a car\\n7. Hitchhike Father\\'s Day was founded in Spokane, Washington. Two Swords, The Lion and the Rose, Breaker of Chains, Oathkeeper, First of His Name, The Laws of Gods and Men, Mockingbird, The Mountain and the Viper, The Watchers on the Wall, The Children Cleats, shin guards, and a water bottle may be helpful things to pack when you’re getting ready for soccer practice. A helmet is not typically used in soccer, a guitar is a musical instrument, and grass is the material soccer is usually played on but not something you’ll need to pack. Yes, Leonidas from Sparta was a real man. He was famous for his courage and leadership during the Battle of Thermopylae in 480 BC. -  Dinner at your favorite pizzeria \\n- A walk around a local park\\n- Mid-afternoon coffee at a local coffee shop\\n- Tour a museum Margaret Mitchell Gurudeb, Kobiguru, Biswokobi. The Northern Colorado Bears teams compete mostly in the Big Sky Conference. However, some teams compete in the Summit League, Western Athletic Conference, and the Big 12 Conference.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_model, get_tokenizer, print_gpu_utilization, time_gpu\n",
    "\n",
    "encode, decode = get_tokenizer(model_type)\n",
    "\n",
    "a = Y.gather(1, torch.tensor(pred_idxs, device=device)).squeeze(1)\n",
    "\n",
    "decode(a.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "from utils import load_model, get_tokenizer, print_gpu_utilization, time_gpu\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = ['resume', 'eval_llama', 'llama', 'gpt2-small', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'][1] # or 'resume' or 'gpt2-medium' or 'gpt2-large' or 'gpt2-xl'\n",
    "out_dir = \"/home/li/basu_workspace/cptData/out/lb2_llama_dolly_0605-0501/ckpt.pt\"\n",
    "start = \"User: Capital of France?\\n\"  # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples =  3  # number of samples to draw\n",
    "max_new_tokens = 200 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# learning block\n",
    "learning_block = True\n",
    "influence = 0.5\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "model_type = 'llama' if 'llama' in init_from else 'gpt2'\n",
    "\n",
    "# sampling = \"continuous\"\n",
    "sampling = \"discrete\"\n",
    "\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "torch.set_default_dtype(ptdtype)\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "\n",
    "# # model\n",
    "# model, model_args = load_model(model_type, out_dir, device, learning_block, influence, init_from)\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# with time_gpu('move model to device'):\n",
    "#     model.to(device)\n",
    "\n",
    "# print_gpu_utilization()\n",
    "\n",
    "# if compile:\n",
    "#     model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "\n",
    "# tokenizer\n",
    "encode, decode = get_tokenizer(model_type)                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_dir = \"/home/li/basu_workspace/nanoGPT/data/dolly\"\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = val_data.reshape(-1, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data[0].tolist()[get_pred_idxs(val_data[0].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_list(sl,l):\n",
    "    results=[]\n",
    "    sll=len(sl)\n",
    "    print(sll)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            results.append((ind,ind+sll-1))\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_pred_idxs(sen):\n",
    "    sep = encode(\"\\nBot: \")[1:-2]\n",
    "    print(sep)\n",
    "    idxs = find_sub_list(sep, sen)\n",
    "    print(\"idxs\", idxs)\n",
    "\n",
    "    pred_idxs = []\n",
    "    for i in range(len(idxs)):\n",
    "        for j in range(idxs[i][1], len(sen)):\n",
    "            if val_data[0][j] == 2:\n",
    "                pred_idxs += [p for p in range(idxs[i][1], j)]     \n",
    "                break\n",
    "            \n",
    "    return pred_idxs\n",
    "\n",
    "# sen = val_data[0].tolist() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating sample 1 of 3\n",
      "User: What is the best food in the world?\n",
      "Bot:  a F This2 alsoB is,7 byB., and can the,:,\n",
      " of D\n",
      " itsB from: to as to What to\n",
      "., are. in there User\n",
      " and a., for a to that to toB\n",
      ") and also following, and- you of.ot the.. aot?:2 by11 will6, on inB as and the in a\n",
      " with, and and of to0: more to, a- of: is9 not be.  of.\n",
      " an: of is The,?\". also, to, the of  in? to the: is you the ( are,4 -\n",
      " of: your theing, to\n",
      " the0 \" in\n",
      "---------------\n",
      "Time taken:  16.523746013641357\n",
      "GPU memory used: 0.01953125\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 12.88GB | 12.81GB | 12.84GB \n",
      "generating sample 2 of 3\n",
      "User: What is the best food in the world?\n",
      "Bot:  the of,. each9 the1 is of are of:5,,,,3?, to by, I ining and of: the.0 into. C in9 a B to,' of6, a.4-,0, of ,,, to::,,B\n",
      "2..),:., to: (, to was of,us to  is and:  to?\n",
      ",\n",
      " a:B9,B of was for,ings\n",
      "1 the, canB from: is,0, UserB in\n",
      ",,2 of then and of. a9 in, (. ,. is-,,: usedB\n",
      ":. Hot\n",
      ". the one is: ofing. the such:\n",
      "---------------\n",
      "Time taken:  17.188156604766846\n",
      "GPU memory used: 0.0\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 12.88GB | 12.81GB | 12.84GB \n",
      "generating sample 3 of 3\n",
      "User: What is the best food in the world?\n",
      "Bot: . and, to'er forB may,.\n",
      " for,B and:\n",
      ", and the of be2: its known. the to ot is the. and a, on, of  also \n",
      " may User by\n",
      " most The we0 and: as is and, and how time your the  is,\n",
      " is, first to\n",
      ".,ot..\n",
      ". a a: - the:\n",
      " knownB The the also for of the from of to be, to is and in the.5B4 B to. to the the up the theing the be), is a: theB the to: It mostB of\n",
      " of in\n",
      ". to. to:.\n",
      " of a:,.8 L the\n",
      " in:ies ofers\n",
      "---------------\n",
      "Time taken:  14.026246786117554\n",
      "GPU memory used: 0.0\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 12.88GB | 12.81GB | 12.84GB \n"
     ]
    }
   ],
   "source": [
    "start = \"User: What is the best food in the world?\\nBot: \"\n",
    "start_ids = encode(start)\n",
    "tkns = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            with time_gpu('generate'):\n",
    "                print(\"generating sample\", k+1, \"of\", num_samples)\n",
    "                y = model.generate(tkns, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "                print(decode(y[0].tolist()))\n",
    "                print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sampling == \"discrete\":\n",
    "    # encode the beginning of the prompt\n",
    "    if start.startswith('FILE:'):\n",
    "        with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "            start = f.read()\n",
    "            \n",
    "    start_ids = encode(start)\n",
    "    tkns = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            for k in range(num_samples):\n",
    "                with time_gpu('generate'):\n",
    "                    print(\"generating sample\", k+1, \"of\", num_samples)\n",
    "                    y = model.generate(tkns, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "                    print(decode(y[0].tolist()))\n",
    "                    print('---------------')\n",
    "\n",
    "\n",
    "if sampling == \"continuous\":\n",
    "\n",
    "    while True:\n",
    "        ## take input\n",
    "        print(\"Enter a sentence to continue:\")\n",
    "        start = str(input())\n",
    "        start_ids = encode(start)\n",
    "        tkns = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "        # run generation\n",
    "        with torch.no_grad():\n",
    "            with ctx:\n",
    "                for k in range(num_samples):\n",
    "                    with time_gpu('Time to generate'):\n",
    "                        print(\"Sample\", k+1, \"------------------------------------\")\n",
    "                        y = model.generate(tkns, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "                        print(decode(y[0].tolist()))\n",
    "                        print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Learning Block True\n",
      "Iterations per epoch: 25\n",
      "Creating and loading model\n",
      "Initializing from OG weights: /home/li/basu_workspace/nanoGPT/../cptData/lit-llama/7B/lit-llama.pth\n",
      "Creating model 123.11571097373962\n",
      "GPU memory used: 0.0\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB |  0.0GB  |  0.0GB  |  0.0GB  \n",
      "Loading state dict 19.42014741897583\n",
      "GPU memory used: 0.0\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB |  0.0GB  |  0.0GB  |  0.0GB  \n",
      "Time to load model:  142.7067472934723\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 184\u001b[0m\n\u001b[1;32m    178\u001b[0m model_args \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(n_layers\u001b[39m=\u001b[39mn_layers, n_heads\u001b[39m=\u001b[39mn_heads, n_embd\u001b[39m=\u001b[39mn_embd, block_size\u001b[39m=\u001b[39mblock_size,\n\u001b[1;32m    179\u001b[0m                   bias\u001b[39m=\u001b[39mbias, vocab_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dropout\u001b[39m=\u001b[39mdropout, learning_block\u001b[39m=\u001b[39mlearning_block) \u001b[39m# start with model_args from command line\u001b[39;00m\n\u001b[1;32m    182\u001b[0m model, model_args \u001b[39m=\u001b[39m load_model(model_type, out_dir, device, learning_block, influence, init_from)\n\u001b[0;32m--> 184\u001b[0m model_copy \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(model)\n\u001b[1;32m    185\u001b[0m \u001b[39mwith\u001b[39;00m time_gpu(device, \u001b[39m'\u001b[39m\u001b[39mmodel to GPU\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    186\u001b[0m     model\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from utils import load_model, Sampler, get_batch, configure_optimizers, time_gpu\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "sample_start = \"User: What is the capital of India.\\nBot:\"\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # or 'resume' or 'gpt2-medium' or 'gpt2-large' or 'gpt2-xl' or 'eval_llama' or 'llama'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = \"transformers\"\n",
    "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'openwebtext'\n",
    "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
    "batch_size = 32 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 2048\n",
    "# model\n",
    "n_layers = 12\n",
    "n_heads = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "max_new_tokens = 10\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "torch.set_default_dtype(ptdtype)\n",
    "\n",
    "# learning block\n",
    "learning_block = False\n",
    "influence = 0.5\n",
    "\n",
    "## instruct\n",
    "data_type = None\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "import time\n",
    "\n",
    "eval_interval = 5\n",
    "eval_iters = 40\n",
    "wandb_log = False # feel free to turn on\n",
    "wandb_project = 'learning-block'\n",
    "\n",
    "\n",
    "wandb_run_name = 'lb2_llama_dolly' + '_' + time.strftime(\"%m%d-%H%M\") ## train_type,  model , dataset\n",
    "dataset = 'dolly'\n",
    "init_from = 'llama'\n",
    "\n",
    "data_type = 'instruct'\n",
    "out_dir = '../cptData/out/' + wandb_run_name \n",
    "\n",
    "# only save checkpoints if the validation loss improves\n",
    "always_save_checkpoint = False\n",
    "\n",
    "# the number of examples per iter:\n",
    "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
    "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
    "batch_size = 1\n",
    "gradient_accumulation_steps = 32\n",
    "max_iters = 100\n",
    "\n",
    "# finetune at constant LR\n",
    "learning_rate = 3e-5\n",
    "decay_lr = False\n",
    "learning_block = True\n",
    "\n",
    "compile = False\n",
    "\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "model_type = 'llama' if 'llama' in init_from else 'gpt2'\n",
    "\n",
    "## if torch version < 2 set compile to False\n",
    "if torch.__version__[0] == '1' and compile:\n",
    "    print(\"PyTorch version < 2.0, disabling compilation\")\n",
    "    compile = False\n",
    "\n",
    "print(\"Using Learning Block\", learning_block)\n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank # each process gets a different seed\n",
    "    assert gradient_accumulation_steps % torch.cuda.device_count() == 0\n",
    "    gradient_accumulation_steps //= torch.cuda.device_count()\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader\n",
    "data_dir = os.path.join(os.path.dirname(os.path.realpath(\"__file__\")), 'data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "print(\"Iterations per epoch:\", train_data.shape[0] // tokens_per_iter)\n",
    "\n",
    "\n",
    "if dataset == 'dolly':\n",
    "    mask_train = np.memmap(os.path.join(data_dir, 'inp_shape_train.bin'), dtype=np.uint16, mode='r')\n",
    "    mask_val = np.memmap(os.path.join(data_dir, 'inp_shape_val.bin'), dtype=np.uint16, mode='r')\n",
    "    train_data = train_data.reshape(-1, block_size)\n",
    "    val_data = val_data.reshape(-1, block_size)\n",
    "    mask_train = mask_train.reshape(train_data.shape[0], -1)\n",
    "    mask_val = mask_val.reshape(val_data.shape[0], -1)\n",
    "    \n",
    "\n",
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layers=n_layers, n_heads=n_heads, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout, learning_block=learning_block) # start with model_args from command line\n",
    "\n",
    "\n",
    "model, model_args = load_model(model_type, out_dir, device, learning_block, influence, init_from)\n",
    "with time_gpu(device, 'model to GPU'):\n",
    "    model.to(device)\n",
    "\n",
    "## set requires grad to false for all layers except learning block\n",
    "\n",
    "if learning_block:\n",
    "    print(\"setting requires_grad=False for all layers except learning block\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if \"learning_block\" not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "## total number of parameters that requires grad\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"total params with requires grad\", total_params/1e6, \"M\")\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "# if init_from == 'resume':\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# compile the model\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n",
    "# wrap model into DDP container\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "sampler = Sampler(model_name = model_type, start = sample_start, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask(inp, seq_len):\n",
    "    mask = np.eye((seq_len))\n",
    "    ptr = 0\n",
    "    for len in inp:\n",
    "        if len == 0:\n",
    "            break\n",
    "        mask[ptr:ptr+len, ptr:ptr+len] = np.tril(np.ones((len, len)))\n",
    "        ptr += len\n",
    "    return mask\n",
    "\n",
    "def get_batch(split, block_size, batch_size, device_type, device, train_data, val_data, data_type=None, mask_train = None, mask_val = None):\n",
    "    \n",
    "    if data_type == None:\n",
    "        data = train_data if split == 'train' else val_data\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "        y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "        \n",
    "        if 'cuda' in device_type:\n",
    "            # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "            x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "        else:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "        return x, y, None\n",
    "    \n",
    "    elif data_type == 'instruct':\n",
    "        \n",
    "        data = train_data if split == 'train' else val_data\n",
    "        mask_data = mask_train if split == 'train' else mask_val\n",
    "        \n",
    "        ix = torch.randint(data.shape[0] - 1, (batch_size,))\n",
    "        \n",
    "        x = torch.stack([torch.from_numpy((data[i][:-1]).astype(np.int64)) for i in ix])\n",
    "        y = torch.stack([torch.from_numpy((data[i+1][1:]).astype(np.int64)) for i in ix])\n",
    "        mask = torch.stack([torch.from_numpy(make_mask(mask_data[i], block_size)[:-1, :-1]) for i in ix])\n",
    "        ## conver mask to bool\n",
    "        mask = mask.bool()\n",
    "        \n",
    "        if 'cuda' in device_type:\n",
    "            # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "            x, y, mask = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True), mask.pin_memory().to(device, non_blocking=True)\n",
    "        else:\n",
    "            x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "        return x, y, mask\n",
    "    \n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Sampling from trained model\")\n",
    "    sampler.generate(model, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in trange(eval_iters):\n",
    "            X, Y, mask = get_batch(split, block_size, batch_size, device_type, device, train_data, val_data, data_type = data_type,\n",
    "                                   mask_train = mask_train, mask_val = mask_val)\n",
    "            with ctx:\n",
    "                logits = model(X, mask=mask)\n",
    "                # print(logits)\n",
    "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=-1)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# logging\n",
    "if wandb_log and master_process:\n",
    "    import wandb\n",
    "    ## add wandb key\n",
    "    wandb_api_key = \"84742742b66deb0de22b5dfec52ec1f23a539d9b\"\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, entity='basujindal123',config=config)\n",
    "\n",
    "# training loop\n",
    "X, Y, mask = get_batch('train', block_size, batch_size, device_type, device, train_data, val_data, data_type = data_type, mask_train = mask_train, mask_val = mask_val)\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
    "iter_num_resume = iter_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from trained model\n",
      "User: What is the capital of India.\n",
      "Bot: *This is impossible.\n",
      "User: What is\n",
      "---------------\n",
      "User: What is the capital of India.\n",
      "Bot: I am sorry, I am not a human.\n",
      "---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:29<00:00,  1.37it/s]\n",
      "100%|██████████| 40/40 [00:35<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ealuate 66.29910826683044\n",
      "GPU memory used: 17.65234375\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 14.61GB | 13.55GB | 14.31GB \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with time_gpu(device,'Ealuate'):\n",
    "    losses = estimate_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "step 0: train loss 12.8125, val loss 12.6875\n",
      "Training Step 49.16111731529236\n",
      "GPU memory used: 17.142578125\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 49.05GB |  30.6GB  | 47.53GB \n",
      "iter 0: loss 18.4256, time 80550.46ms\n",
      "Training Step 47.726683139801025\n",
      "GPU memory used: 0.013671875\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 49.05GB |  30.6GB  | 47.54GB \n",
      "iter 1: loss 12.0520, time 47748.21ms\n",
      "Training Step 48.63527154922485\n",
      "GPU memory used: 4.2880859375\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 49.05GB |  30.6GB  | 47.54GB \n",
      "iter 2: loss 11.7840, time 48774.90ms\n",
      "Training Step 47.903974771499634\n",
      "GPU memory used: 4.013671875\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 49.05GB |  30.6GB  | 47.54GB \n",
      "iter 3: loss 10.4763, time 48111.75ms\n",
      "Training Step 47.782862186431885\n",
      "GPU memory used: 0.0\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 49.05GB |  30.6GB  | 47.54GB \n",
      "iter 4: loss 11.6493, time 47795.05ms\n",
      "step 5: train loss 12.8125, val loss 12.6875\n",
      "Training Step 16.23002028465271\n",
      "GPU memory used: 0.0\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 79.21GB | 49.05GB | 30.87GB | 47.54GB \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[39m# immediately async prefetch next batch while model is doing the forward pass on the GPU\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     X, Y, mask \u001b[39m=\u001b[39m get_batch(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, block_size, batch_size, device_type, device, train_data, val_data, \n\u001b[1;32m     54\u001b[0m                            data_type \u001b[39m=\u001b[39m data_type, mask_train \u001b[39m=\u001b[39m mask_train, mask_val \u001b[39m=\u001b[39m mask_val)\n\u001b[0;32m---> 55\u001b[0m     scaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     56\u001b[0m \u001b[39m# clip the gradient\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m grad_clip \u001b[39m!=\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda/envs/basu/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda/envs/basu/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "for iter_num in range(iter_num_resume, max_iters+1):\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        \n",
    "        with time_gpu(device,'Ealuate'):\n",
    "            losses = estimate_loss()\n",
    "            \n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "            })\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    # 'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir,'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    with time_gpu(device, 'Training Step'):\n",
    "        \n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            if ddp:\n",
    "                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "            with ctx:\n",
    "                logits = model(X, mask=mask)\n",
    "                ## check if any nan values in logits\n",
    "                if torch.isnan(logits).any():\n",
    "                    raise ValueError('NaN values in logits')\n",
    "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=-1)\n",
    "                loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "                \n",
    "            # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "            X, Y, mask = get_batch('train', block_size, batch_size, device_type, device, train_data, val_data, \n",
    "                                   data_type = data_type, mask_train = mask_train, mask_val = mask_val)\n",
    "            scaler.scale(loss).backward()\n",
    "        # clip the gradient\n",
    "        if grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import sys\n",
    "from llamaTokenizer import LLaMAtokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokenizer_path = \"/home/li/basu_workspace/llama/tokenizer.model\"\n",
    "tokenizer = LLaMAtokenizer(model_path=tokenizer_path)\n",
    "\n",
    "enc = lambda s: tokenizer.encode(s, bos=False, eos=True)\n",
    "dec = lambda s: tokenizer.decode(s)\n",
    "\n",
    "import json\n",
    "\n",
    "with open('/home/li/basu_workspace/nanoGPT/data/dolly/databricks-dolly-15k.jsonl') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "data = [json.loads(line) for line in data]\n",
    "data_cleaned  = [\"User: \" + instruct['instruction'] + \"\\nBot: \" + instruct['response'] for instruct in data]\n",
    "\n",
    "encoded = []\n",
    "for sentence in data_cleaned:\n",
    "    encoded.append(enc(sentence))\n",
    "assert len (encoded) == len(data_cleaned)\n",
    "\n",
    "seq_len = 2048\n",
    "### create new encoded if len(encoded[i]) < seq_len\n",
    "\n",
    "encoded = [encoded[i] for i in range(len(encoded)) if len(encoded[i]) < seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = np.ones((len(encoded), seq_len), dtype=np.int32)*2\n",
    "j = 0\n",
    "k = 0\n",
    "sen_lens = []\n",
    "l = []\n",
    "for i in encoded:\n",
    "    if k + len(i) > seq_len:\n",
    "        sen_lens.append(l)\n",
    "        l = [len(i)]\n",
    "        j+=1\n",
    "        k=len(i)\n",
    "    else:\n",
    "        k+=len(i)\n",
    "        l.append(len(i))\n",
    "    comb[j, k-len(i):k] = i\n",
    "\n",
    "for i in range(len(comb)):\n",
    "    if np.all(comb[i] == 2):\n",
    "        num_sen = i-1\n",
    "        break\n",
    "\n",
    "comb = comb[:num_sen]\n",
    "\n",
    "max_len = max([len(i) for i in sen_lens])\n",
    "## pad sen_lens with zeros\n",
    "sen_lens = [i + [0]*(max_len - len(i)) for i in sen_lens]\n",
    "sen_lens = [item for sublist in sen_lens for item in sublist]\n",
    "\n",
    "train_frac = 0.9\n",
    "train_ids = comb[:int(train_frac*len(comb))]\n",
    "val_ids = comb[int(train_frac*len(comb)):]\n",
    "\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "sen_lens = np.array(sen_lens, dtype=np.uint16)\n",
    "train_ids.tofile('/home/li/basu_workspace/nanoGPT/data/dolly/train.bin')\n",
    "val_ids.tofile( '/home/li/basu_workspace/nanoGPT/data/dolly/val.bin')\n",
    "sen_lens.tofile('/home/li/basu_workspace/nanoGPT/data/dolly/sen_lens.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/li/basu_workspace/nanoGPT/data/databricks-dolly-15k.jsonl') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "data = [json.loads(line) for line in data]\n",
    "data_cleaned  = [\"User: \" + instruct['instruction'] + \"\\nBot: \" + instruct['response'] for instruct in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = []\n",
    "for sentence in data_cleaned:\n",
    "    encoded.append(enc(sentence))\n",
    "assert len (encoded) == len(data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(encoded[i]) for i in range(len(encoded))]\n",
    "sorted_lens = sorted(lens)\n",
    "pl = plt.hist(sorted_lens[:-200], bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import safetensors\n",
    "from safetensors.torch import save_model\n",
    "\n",
    "save_model(model, 'my_model.safetensors')\n",
    "\n",
    "# tensors = {}\n",
    "# with safe_open(\"model.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "#    for key in f.keys():\n",
    "#        tensors[key] = f.get_tensor(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import safetensors\n",
    "safetensors.torch.load_model(model, 'my_model.safetensors')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
