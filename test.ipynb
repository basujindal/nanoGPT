{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from OG weights: /home/li/basu_workspace/nanoGPT/lit-llama/7B/lit-llama.pth\n",
      "{'n_layers': 32, 'n_heads': 32, 'learning_block': True, 'influence': 0, 'vocab_size': 32000, 'max_seq_len': 2048, 'n_embed': 4096}\n",
      "LLaMA(\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(32000, 4096)\n",
      "    (h): ModuleList(\n",
      "      (0-31): 32 x Block(\n",
      "        (rms_1): RMSNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=4096, out_features=12288, bias=False)\n",
      "          (c_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (rms_2): RMSNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (c_fc2): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (c_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): RMSNorm()\n",
      "  )\n",
      ")\n",
      "<function <lambda> at 0x7f023e7fa0d0>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from utils import load_model\n",
    "from llamaTokenizer import LLaMAtokenizer\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = ['resume', 'llama', 'gpt2-small', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'][1] # or 'resume' or 'gpt2-medium' or 'gpt2-large' or 'gpt2-xl'\n",
    "out_dir = '/home/li/basu_workspace/nanoGPT/harrypotter-learning-block_1684388718.5518227' # ignored if init_from is not 'resume'\n",
    "start = \"User: Capital of France? \\n Bot: Paris \\n User: Capital of India \\n Bot:\"  # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples =  3  # number of samples to draw\n",
    "max_new_tokens = 100 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# learning block\n",
    "learning_block = True\n",
    "influence = 0\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "model_type = 'llama' if 'llama' in init_from else 'gpt2'\n",
    "\n",
    "# sampling = \"continuous\"\n",
    "sampling = \"discrete\"\n",
    "\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# model\n",
    "model, model_args = load_model(model_type, out_dir, device, learning_block, influence, init_from)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(model)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "\n",
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "else:\n",
    "\n",
    "    if model_type == 'gpt2':\n",
    "        # ok let's assume gpt-2 encodings by default\n",
    "        print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "        enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "        decode = lambda l: enc.decode(l)\n",
    "\n",
    "    elif model_type == 'llama':\n",
    "        tokenizer_path = \"/home/li/basu_workspace/llama/tokenizer.model\"\n",
    "        tokenizer = LLaMAtokenizer(model_path=tokenizer_path)\n",
    "        encode = lambda s: tokenizer.encode(s, bos=True, eos=False)\n",
    "        print(encode)\n",
    "        decode = lambda l: tokenizer.decode(l)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if start.startswith('FILE:'):\n",
    "#     with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "#         start = f.read()\n",
    "\n",
    "start = \"The capital of France is \" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "\n",
    "model.eval()\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            print(\"generating sample\", k+1, \"of\", num_samples)\n",
    "            # model.reset_cache()\n",
    "            start_ids = encode(start)\n",
    "            print(start_ids)\n",
    "            tkns = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "            y = model.generate(tkns, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4911, 29901, 25343, 310, 3444, 29973, 29871, 13, 11273, 29901, 3681, 29871, 13, 4911, 29901, 25343, 310, 7513, 29871, 13, 11273, 29901] \n",
      " User: Capital of France? \n",
      " Bot: Paris \n",
      " User: Capital of India \n",
      " Bot:\n",
      "torch.Size([1, 23, 4096])\n",
      "torch.Size([1, 32000])\n",
      "[1570]\n",
      "New\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "start = \"User: Capital of France? \\n Bot: Paris \\n User: Capital of India \\n Bot:\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "\n",
    "start_ids = encode(start)   \n",
    "print(start_ids, '\\n', decode(start_ids))\n",
    "tkns = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "model.reset_cache()\n",
    "logits,_  = model(tkns)\n",
    "\n",
    "logits = logits[:, -1, :]\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print(probs.shape)\n",
    "# sample from the distribution\n",
    "# idx_next = torch.multinomial(probs, num_samples=1)\n",
    "idx_next = torch.argmax(probs, dim=-1)\n",
    "\n",
    "print(idx_next.tolist())\n",
    "print(decode(idx_next.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sampling == \"discrete\":\n",
    "    # encode the beginning of the prompt\n",
    "    if start.startswith('FILE:'):\n",
    "        with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "            start = f.read()\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            for k in range(num_samples):\n",
    "                print(\"generating sample\", k+1, \"of\", num_samples)\n",
    "\n",
    "                start_ids = encode(start)\n",
    "                tkns = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "                y = model.generate(tkns, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "                print(decode(y[0].tolist()))\n",
    "                print('---------------')\n",
    "\n",
    "\n",
    "if sampling == \"continuous\":\n",
    "\n",
    "    while True:\n",
    "        ## take input\n",
    "        print(\"Enter a sentence to continue:\")\n",
    "        start = str(input())\n",
    "        start_ids = encode(start)\n",
    "        tkns = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "        # run generation\n",
    "        with torch.no_grad():\n",
    "            with ctx:\n",
    "                for k in range(num_samples):\n",
    "                    print(\"Sample\", k+1, \"------------------------------------\")\n",
    "                    \n",
    "\n",
    "                    if model_type == 'llama':\n",
    "                        y = model.generate(prompts = tkns, max_new_tokens = max_new_tokens, tokenizer=tokenizer, temperature=temperature)\n",
    "                        print(y)\n",
    "                    else:\n",
    "                        y = model.generate(tkns, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "                        print(decode(y[0].tolist()))\n",
    "                    print('---------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
