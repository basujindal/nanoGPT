{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/li/basu_workspace/nanoGPT\n",
      "Number of examples in alpaca:  51760\n",
      "Number of examples in dolly:  15011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66771/66771 [00:28<00:00, 2327.53it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from tqdm import trange\n",
    "\n",
    "pth = os.path.abspath(os.path.dirname(os.path.dirname(os.path.dirname(\"__file__\"))))\n",
    "\n",
    "print(pth)\n",
    "###################################################################################################                          \n",
    "\n",
    "## alpaca cleaned\n",
    "with open(os.path.join(pth, 'data/alpaca/alpaca_data_cleaned.json')) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "data_cleaned  = [\"###User: \" + instruct['input'] + \"\\n\" + instruct['instruction'] + \"\\n###Bot: \" + instruct['output'] for instruct in data]\n",
    "\n",
    "print(\"Number of examples in alpaca: \", len(data_cleaned))\n",
    "\n",
    "## dolly\n",
    "with open(os.path.join(pth, 'data/dolly/databricks-dolly-15k.jsonl')) as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "data = [json.loads(line) for line in data]\n",
    "data_cleaned_dolly  = [\"###User: \" + instruct['context'] + \"\\n\" +  instruct['instruction'] + \"\\n###Bot: \" + instruct['response'] for instruct in data]\n",
    "\n",
    "print(\"Number of examples in dolly: \", len(data_cleaned_dolly))\n",
    "\n",
    "data_cleaned += data_cleaned_dolly\n",
    "\n",
    "# ## gpt4all\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# data = load_dataset(\"nomic-ai/gpt4all-j-prompt-generations\", revision='v1.3-groovy')\n",
    "# data_cleaned_gpt4all  = [\"###User: \" + instruct['prompt'] + \"\\n\\n###Bot: \" + instruct['response'] for instruct in data['train']]\n",
    "\n",
    "# print(\"Number of examples in gpt4all: \", len(data_cleaned_gpt4all))\n",
    "\n",
    "# data_cleaned += data_cleaned_gpt4all\n",
    "\n",
    "# print(\"Total number of examples: \", len(data_cleaned))\n",
    "############################################################################################################\n",
    "\n",
    "sys.path.append(pth)\n",
    "from llamaTokenizer import LLaMAtokenizer\n",
    "\n",
    "tokenizer_path = os.path.join(os.path.dirname(pth), \"cptData/lit-llama/tokenizer.model\")\n",
    "\n",
    "train_frac = 0.9\n",
    "seq_len = 2048\n",
    "dataset = 'instruct2'\n",
    "\n",
    "tokenizer = LLaMAtokenizer(model_path=tokenizer_path)\n",
    "enc = lambda s: tokenizer.encode(s, bos=False, eos=True)\n",
    "dec = lambda s: tokenizer.decode(s)\n",
    "\n",
    "encoded = [enc(data_cleaned[i]) for i in trange(len(data_cleaned))]\n",
    "assert len (encoded) == len(data_cleaned)\n",
    "\n",
    "encoded = [encoded[i] for i in range(len(encoded)) if len(encoded[i]) < seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total=0\n",
    "for i in encoded:\n",
    "    num_total+=len(i)\n",
    "num_total = num_total//2048 + 1\n",
    "comb = np.ones((num_total, seq_len), dtype=np.int32)*2 ## pad with eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66717/66717 [00:00<00:00, 109720.24it/s]\n"
     ]
    }
   ],
   "source": [
    "j, k = 0, 0\n",
    "\n",
    "for i in trange(len(encoded)):\n",
    "    sen = encoded[i]\n",
    "    \n",
    "    if k + len(sen) > seq_len:\n",
    "\n",
    "        remlen = seq_len - k\n",
    "        comb[j, -remlen:] = sen[:remlen]\n",
    "        \n",
    "        k = len(sen) - remlen\n",
    "        l = [k]\n",
    "        j+=1\n",
    "        comb[j, :k] = sen[-k:]\n",
    "        \n",
    "    else:\n",
    "        comb[j, k:len(sen)+k] = sen\n",
    "        k+=len(sen)\n",
    "    if k == seq_len:\n",
    "        k = 0\n",
    "        j+=1\n",
    "        \n",
    "sen_lens = []\n",
    "for i in comb:\n",
    "    idx_old = 0\n",
    "    l = []\n",
    "    for idx, j in enumerate(i):\n",
    "        if j == 2:\n",
    "            l.append(idx-idx_old + 1)\n",
    "            idx_old = idx + 1\n",
    "    l.append(len(i)-idx_old)\n",
    "    sen_lens.append(l)\n",
    "\n",
    "all_lens = []\n",
    "for i in sen_lens:\n",
    "    all_lens+=i\n",
    "\n",
    "j = 0\n",
    "for i in range(len(encoded)):\n",
    "    if len(encoded[i]) == all_lens[j]:\n",
    "        j+=1\n",
    "    else:\n",
    "        if len(encoded[i]) != all_lens[j] + all_lens[j+1]:\n",
    "            assert False\n",
    "        else:\n",
    "            j+=2\n",
    "            \n",
    "all_lens = all_lens[:j]\n",
    "            \n",
    "anss = 0\n",
    "for i in range(len(encoded)):\n",
    "    anss += len(encoded[i])\n",
    "    \n",
    "    \n",
    "ans = 0\n",
    "for j in all_lens:\n",
    "    ans+=j\n",
    "\n",
    "assert ans == anss\n",
    "\n",
    "for i in range(len(sen_lens[-1])):\n",
    "    if sen_lens[-1][i] == 1:\n",
    "        ans = i\n",
    "        break\n",
    "sen_lens[-1] = sen_lens[-1][:ans]\n",
    "sen_lens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'###User: \\nGive three tips for staying healthy.\\n###Bot: 1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night. ###User: \\nWhat are the three primary colors?\\n###Bot: The three primary colors are red, blue, and yellow. These colors are called primary because they cannot be created by mixing other colors and all other colors can be made by combining them in various proportions. In the additive color system, used for light, the primary colors are red, green, and blue (RGB). ###User: \\nDescribe the structure of an atom.\\n###Bot: An atom is the basic building block of all matter and is made up of three types of particles: protons, neutrons, and electrons. The structure of an atom can be described as a nucleus at the center surrounded by a cloud of electrons.\\n\\nThe nucleus of an atom is made up of protons and neutrons. Protons are positively charged particles and neutrons are neutral particles with no charge. Both of these particles are located in the nucleus of the atom, which is at the center of the atom and contains most of the atom\\'s mass.\\n\\nSurrounding the nucleus of the atom is a cloud of electrons. Electrons are negatively charged particles that are in constant motion around the nucleus. The electron cloud is divided into shells or orbitals, and each shell can hold a certain number of electrons. The number of electrons in the outermost shell, called the valence shell, determines the chemical properties of the atom. \\n\\nIn a neutral atom, the number of protons in the nucleus is equal to the number of electrons in the electron cloud, so the positive and negative charges balance out and the atom has no overall charge. The number of protons, also called the atomic number, determines what element the atom is. ###User: \\nHow can we reduce air pollution?\\n###Bot: There are several ways to reduce air pollution, including:\\n\\n1. Reduce energy consumption: By conserving energy, we reduce the amount of pollution emitted from power plants. Some ways to achieve this include: using energy-efficient appliances and lighting, insulating homes, and shutting off lights and electronics when not in use.\\n\\n2. Use public transportation or carpool: Cars and trucks are major sources of pollution. By using public transportation, carpooling or biking, you can reduce the number of cars on the road, which in turn reduces emissions.\\n\\n3. Be responsible with waste: When waste is burned or decomposed, they can release harmful gases into the air. Dispose of hazardous waste correctly and recycle materials like plastics, paper, and glass when possible.\\n\\n4. Support clean energy sources: Renewable energy sources like solar and wind power generate little or no pollution. Support clean energy companies and projects, and consider installing renewable energy systems in your home.\\n\\n5. Drive efficiently: If you need to drive, make sure your vehicle is well maintained and drive efficiently by avoiding rapid acceleration and braking, and reducing idling.\\n\\n6. Be mindful of product choices: Some products and practices release more pollution than others. Choose products with eco-friendly packaging and try to minimize your use of aerosol sprays, which can release harmful chemicals into the air. ###User: \\nPretend you are a project manager of a construction company. Describe a time when you had to make a difficult decision.\\n###Bot: I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities. ###User: \\nWrite a concise summary of the following:\\n\"Commodore 64 (commonly known as the C64 or CBM 64) was manufactured by Commodore Business Machine (CBM) in August 1982 with a starting price of $595. It was an 8-bit home computer with remarkable market success. Between 1983-1986, C64 sales amounted to about 17 million units sold, making them the best-selling single personal computer model of all time in 1983-1986. \\n\\nAdditionally, the Commodore 64 dominated the market with between 30% and 40% share and 2 million units sold per year, outselling the IBM PC clones, Apple Computers, and Atari computers. Adding to their success, Sam Tramiel (former Atari president), during an interview in 1989, said they were building 400,000 C64s a month for a couple of years. \"\\n###Bot: The Commodore 64 was a highly successful 8-bit home computer manufactured by Commodore Business Machine (CBM) in 1982, with sales amounting to approximately 17 million units sold between 1983-1986. It dominated the market with between 30% and 40% share and outsold its competitors, including IBM PC clones, Apple Computers, and Atari computers. At its peak, CBM was building 400,000 C64s a month for a couple of years. ###User: 4/16\\nExplain why the following fraction is equivalent to 1/4\\n###Bot: The fraction 4/16 is equivalent to 1/4 because both fractions represent the same value. A fraction can be simplified by dividing both the numerator and the denominator by a common factor. In this case, 4 is a common factor of both the numerator and the denominator of 4/16. When we divide both by 4, we get 4/4 = 1 and 16/4 = 4, so the simplified fraction is 1/4. Alternatively, we can think of this in terms of multiplication. For example, if we multiply the numerator and denominator of the fraction 1/4 by 4, we get (1x4)/(4x4), or 4/16. Since both fractions can be derived from the other through multiplication or division by the same number, they represent the same value and are equivalent. ###User: \\nWrite a short story in third person narration about a protagonist who has to make an important career decision.\\n###Bot: Sophie sat at her desk, staring blankly at the computer screen. Her mind was racing as she weighed the options in front of her. She had been offered a promotion to a senior management position within her company, but it meant moving across the country and leaving her friends and family behind. At the same time, her dream job at a rival company had finally opened up, and she had been invited for an interview. \\n\\nThe thought of leaving her comfortable life behind, to take a chance on something new and uncertain, made Sophie feel anxious, but also excited. She had always been one to take risks, and it had paid off in the past. But now, with such a big decision to make, Sophie couldn\\'t help but feel overwhelmed.\\n\\nAfter much deliberation, Sophie made her choice. She packed her bags, said goodbye to her loved ones, and moved across the country for the new job. It was a difficult first few months, adjusting to a new city and a new company culture. But as time went by, Sophie realized that she had made the best decision for her career. The job was challenging, but also rewarding, and she was surrounded by passionate, driven colleagues who inspired her to grow and develop.\\n\\nIn the end, Sophie\\'s bold move paid off. She was thriv'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(comb[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ing in her new role, and had made new friends and connections in her new city. Looking back, Sophie was grateful for the opportunity to take a risk and make a change, and was confident that she had made the right decision for her career. ###User: He finnished his meal and left the resturant\\nEvaluate this sentence for spelling and grammar mistakes\\n###Bot: There are two spelling errors in the sentence. The corrected sentence should be: \"He finished his meal and left the restaurant.\" ###User: \\nHow did Julius Caesar die?\\n###Bot: Julius Caesar, the Roman Military general, and statesman was assassinated on March 15, 44 BC. He was stabbed to death at a meeting of the Senate in the Theatre of Pompey by a group of senators who called themselves the Liberatores. Led by Gaius Cassius Longinus, and Marcus Junius Brutus, they believed that Caesar\\'s dictatorship was subverting the Roman Republic and sought to restore the traditional republican government. The exact number of assailants is not known, but it is believed that a group of as many as 60 conspirators participated in the assassination, and each of them took turns in stabbing Caesar. The day of his death is still known as the Ides of March. ###User: \\nWhat is the capital of France?\\n###Bot: The capital city of France is Paris. ###User: \\nGenerate a list of ten items a person might need for a camping trip\\n###Bot: Here are ten items a person might need for a camping trip:\\n\\n1. Tent - to provide shelter and protection from the elements\\n2. Sleeping bag - to stay warm and comfortable while sleeping\\n3. Portable stove or campfire grill - for cooking meals\\n4. Cooler with ice or ice packs - to keep perishable food and drinks cold\\n5. Lantern or flashlight - to provide light during the night\\n6. First aid kit - for minor injuries or ailments\\n7. Map and compass or GPS - for navigation on hikes or exploring the area\\n8. Camp chairs or folding chairs - for comfortable seating around the campsite\\n9. Insect repellent - to protect against bug bites\\n10. Sunscreen - to protect against sunburn. ###User: \\nDiscuss the causes of the Great Depression\\n###Bot: The Great Depression was a period of economic decline that lasted from 1929-1939, making it the longest-lasting depression in modern history. It affected not only the United States but countries around the world, causing substantial social and economic upheaval.\\n\\nThere were several major causes of the Great Depression, which include:\\n\\n1. Stock Market Crash of 1929 - In October of 1929, the stock market experienced a significant fall that wiped out millions of investors. This event is considered by many to be the initial trigger of the Great Depression.\\n\\n2. Overproduction - During the 1920s, many industries produced more goods than consumers wanted or could afford. This ultimately led to a decline in demand for goods, causing job loss, lower wages, and business failure.\\n\\n3. High Tariffs and War Debts - Protectionist trade policies in the form of high tariffs led to a decline in global trade, as other countries retaliated with tariffs of their own. Additionally, many countries were struggling to repay war debts, which led to economic instability.\\n\\n4. Bank Failures - As demand for goods declined, many banks began to fail, causing a loss of confidence in the banking system. This led to a massive withdrawal of money from banks, causing even more banks to fail.\\n\\n5. Drought Conditions - The Dust Bowl was a severe drought and dust storm that hit the Great Plains region of the United States in the 1930s. This had a significant impact on agriculture, causing many farmers to lose their land and livelihoods which worsened the effects of the depression.\\n\\nOverall, the Great Depression was caused by a combination of factors, including economic, environmental, and political factors. Its impact was widespread, affecting millions of people around the world. ###User: Oak tree, copper ore, elephant\\nClassify the following into animals, plants, and minerals\\n###Bot: Animals: Elephant\\nPlants: Oak tree\\nMinerals: Copper ore ###User: \\nExplain the use of word embeddings in Natural Language Processing\\n###Bot: Word embeddings are a type of natural language processing technique used to map words or phrases from a vocabulary to vectors of real numbers. The idea is to represent words in a continuous vector space, where the similarity between words can be measured by the distance between their corresponding vectors. This can be done using algorithms such as Word2Vec or GloVe, among others.\\n\\nThe use of word embeddings has revolutionized the field of Natural Language Processing, as it allows computers to understand text in a much more meaningful way than simply looking at the presence or absence of individual words. For example, when using word embeddings, the computer can understand that the words \"dog\" and \"puppy\" are closely related, while the words \"dog\" and \"umbrella\" are not.\\n\\nWord embeddings are used in a wide variety of NLP tasks, including text classification, sentiment analysis, machine translation, named entity recognition, and many others. They form the basis for many advanced language processing techniques, such as deep learning and neural machine translation. ###User: \\nDescribe the function of a computer motherboard\\n###Bot: The motherboard, also known as the mainboard or system board, is the central printed circuit board in a computer. It serves as the backbone or foundation for a computer, connecting all the different components such as the CPU, RAM, storage drives, expansion cards, and peripherals. The motherboard manages communication and data transfer between these components, allowing them to work together and perform their designated tasks.\\n\\nThe motherboard also includes important circuitry such as the power regulation circuit that provides power to the different components, and the clock generator which synchronizes the operation of these components. It also contains the BIOS (basic input/output system), which is a firmware that controls the boot process and provides an interface for configuring and managing the computer\\'s hardware. Other features on a motherboard may include built-in networking, audio, and video capabilities.\\n\\nOverall, the function of a computer motherboard is to provide a platform for the integration and operation of all the various components that make up a computer, making it an essential part of any computing system. ###User: def factorialize(num):\\n  factorial = 1\\n  for i in range(1, num):\\n    factorial *= i\\n  \\n  return factorial\\nRefactor this code and add comments.\\n###Bot: ```\\n# This is a function that calculates the factorial of a given number\\ndef factorialize(num):\\n  # Set the variable \\'factorial\\' to 1\\n  factorial = 1\\n  # Iterate from 1 to \\'num\\' using the range function\\n  for i in range(1, num):\\n    # Multiply the \\'factorial\\' variable by the current number in the iteration\\n    factorial *= i\\n  # Return the final value of \\'factorial\\'\\n  return factorial\\n``` ###User: \\nPropose an ethical solution to the problem of data privacy\\n###Bot: One ethical solution to the problem of data privacy is to enforce stricter regulations and standards that require companies to be transparent about how they collect, store, and use user data. This means that companies must explicitly state what data they are collecting from users, why they need this data, and how they plan to use it. Users will also have the right to access, modify, and delete their data at any time.\\n\\nIn addition, companies should prioritize implementing robust security measures to protect user data from unauthorized access and breaches. This could include measures such as encryption, multi-factor authentication, and regular security audits. In the event of a data breach, companies must be held accountable and must inform affected users in a timely manner.\\n\\nMoreover, users should be empowered to make informed decisions about their data and privacy. This can be achieved by raising awareness about data privacy and providing tools and resources to help individuals better manage their personal information online.\\n\\nUltimately, ensuring data privacy is a shared responsibility between companies and users. Companies must be committed to safeguarding user data, while users must take an active role in protecting their personal information. By working together, we can create a safer and more ethical data landscape. ###User: \\nGenerate three verbs that mean the same as \"to apologize\"\\n###Bot: 1. To express regret\\n2. To atone\\n3'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(comb[1].tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(i) for i in sen_lens])\n",
    "\n",
    "sen_lens = [i + [0]*(max_len - len(i)) for i in sen_lens]\n",
    "\n",
    "inp_shape_train = sen_lens[:int(train_frac*len(comb))]\n",
    "inp_shape_val = sen_lens[int(train_frac*len(comb)):]\n",
    "\n",
    "train_ids = comb[:int(train_frac*len(comb))]\n",
    "val_ids = comb[int(train_frac*len(comb)):]\n",
    "\n",
    "assert train_ids.shape[0] == len(inp_shape_train)\n",
    "assert val_ids.shape[0] == len(inp_shape_val)\n",
    "\n",
    "print(\"train_ids.shape: \", train_ids.shape)\n",
    "print(\"val_ids.shape: \", val_ids.shape)\n",
    "\n",
    "inp_shape_train = [item for sublist in inp_shape_train for item in sublist]\n",
    "inp_shape_val = [item for sublist in inp_shape_val for item in sublist]\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "inp_shape_train = np.array(inp_shape_train, dtype=np.uint16)\n",
    "inp_shape_val = np.array(inp_shape_val, dtype=np.uint16)\n",
    "\n",
    "train_ids.tofile(os.path.join(pth, 'data/{dataset}/train.bin'.format(dataset=dataset)))\n",
    "val_ids.tofile(os.path.join(pth, 'data/{dataset}/val.bin'.format(dataset=dataset)))\n",
    "inp_shape_train.tofile(os.path.join(pth, 'data/{dataset}/inp_shape_train.bin'.format(dataset=dataset)))\n",
    "inp_shape_val.tofile(os.path.join(pth, 'data/{dataset}/inp_shape_val.bin'.format(dataset=dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poor man's data loader\n",
    "\n",
    "import os   \n",
    "import numpy as np\n",
    "dataset = 'dolly'\n",
    "block_size = 2048\n",
    "\n",
    "\n",
    "data_dir = os.path.join(os.path.dirname(os.path.realpath(\"__file__\")), 'data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "# print(\"Iterations per epoch:\", train_data.shape[0] // tokens_per_iter)\n",
    "\n",
    "\n",
    "# # if data_type == 'instruct':\n",
    "# mask_train = np.memmap(os.path.join(data_dir, 'inp_shape_train.bin'), dtype=np.uint16, mode='r')\n",
    "# mask_val = np.memmap(os.path.join(data_dir, 'inp_shape_val.bin'), dtype=np.uint16, mode='r')\n",
    "# train_data = train_data.reshape(-1, block_size)\n",
    "# val_data = val_data.reshape(-1, block_size)\n",
    "# mask_train = mask_train.reshape(train_data.shape[0], -1)\n",
    "# mask_val = mask_val.reshape(val_data.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'###User: <p>Sorry for the easy question I am very new to IOS developing. What I would like to do is make the UILabel which is set to 0 go up by 1 every time a button is tapped on. Here is the default code given On the ViewController.swift and I have created an Outlet for the Label and Action for the Button. </p>\\n\\n<pre><code>import UIKit\\n\\nclass ViewController: UIViewController {\\n@IBOutlet var score: UILabel!\\n\\n@IBAction func button(sender: UIButton) {\\n\\n\\n}\\n\\noverride func viewDidLoad() {\\n    super.viewDidLoad()\\n\\n}\\n\\noverride func didReceiveMemoryWarning() {\\n    super.didReceiveMemoryWarning()\\n\\n}\\n\\n\\n}\\n</code></pre>\\n###Bot: No need to apologize for asking a question! Here\\'s how you can make the UILabel go up by 1 each time the button is tapped:\\n\\n1. Create a variable to keep track of the score:\\n\\n```\\nvar currentScore = 0\\n```\\n\\n2. In the `button` function, increment the `currentScore` variable and update the label text:\\n\\n```\\n@IBAction func button(sender: UIButton) {\\n    // Increment the score by 1\\n    currentScore += 1\\n    \\n    // Update the label text to show the current score\\n    score.text = \"\\\\(currentScore)\"\\n}\\n```\\n\\nThe `score.text = \"\\\\(currentScore)\"` line converts the `currentScore` variable to a string and sets the label\\'s text property to the resulting string. This updates the label to show the current score.\\n\\nHope that helps! Let me know if you have any questions. ###User: <p>I know the formula of finding minimum number of node in a AVL tree is </p>\\n\\n<p><code>S(h) = S(h-1) + S(h-2) + 1</code></p>\\n\\n<p>However, I don\\'t really get how to use this function, say if we have a AVL height of 6.\\nThe answer tells me that Minimum = 7 + 4 + 1 =12. But how do you get this number? I mean when you plug in 6 isn\\'t it (6-1) + (6-2) + 1?</p>\\n\\n<p>Can anyone explain to me how to solve this? My teacher haven\\'t talk about this yet but I really want to figure this out myself in order to be prepared for the test next week.</p>\\n###Bot: <p>The formula for finding the minimum number of nodes in an AVL tree of height <code>h</code> is:</p>\\n\\n<p><code>S(h) = S(h-1) + S(h-2) + 1</code></p>\\n\\n<p>where <code>S(h)</code> is the minimum number of nodes needed to construct an AVL tree of height <code>h</code>.</p>\\n\\n<p>To use this formula, you need to work iteratively from the base cases of <code>S(0)</code> and <code>S(1)</code>. It\\'s easiest to start by defining <code>S(0)</code> and <code>S(1)</code> to be 1:</p>\\n\\n<ul>\\n<li><code>S(0) = 1</code></li>\\n<li><code>S(1) = 1</code></li>\\n</ul>\\n\\n<p>Then, you can use the formula to find <code>S(2)</code>:</p>\\n\\n<ul>\\n<li><code>S(2) = S(1) + S(0) + 1 = 3</code></li>\\n</ul>\\n\\n<p>You can continue this process to find <code>S(3)</code>, <code>S(4)</code>, and so on:</p>\\n\\n<ul>\\n<li><code>S(3) = S(2) + S(1) + 1 = 5</code></li>\\n<li><code>S(4) = S(3) + S(2) + 1 = 9</code></li>\\n<li><code>S(5) = S(4) + S(3) + 1 = 15</code></li>\\n</ul>\\n\\n<p>Once you have <code>S(h)</code> for a particular value of <code>h</code>, you can use the formula to find the minimum number of nodes needed to construct an AVL tree of that height. For example, to find the minimum number of nodes needed to construct an AVL tree of height 6, you can use the formula:</p>\\n\\n<ul>\\n<li><code>S(6) = S(5) + S(4) + 1 = 25</code></li>\\n</ul>\\n\\n<p>So the minimum number of nodes needed to construct an AVL tree of height 6 is 25.</p>\\n\\n<p>However, the question asks for the minimum number of nodes needed to construct an AVL tree of height 6, not the general formula for calculating <code>S(h)</code>. To answer this specific question, you can simply plug in <code>h = 6</code> into the formula:</p>\\n\\n<ul>\\n<li><code>S(6) = S(5) + S(4) + 1</code></li>\\n<li><code>S(6) = 15 + 9 + 1 = 25</code></li>\\n</ul>\\n\\n<p>So the answer is 25, not 12.</p> ###User: <p>I have a list of many Python objects like this:</p>\\n\\n<pre><code>class RangeClass(object):\\n\\n    def __init__(self,address,size):\\n        self.address=address\\n        self.size=size\\n        #other attributes and methods...\\n</code></pre>\\n\\n<p>Then, I have a list (rangelist) of RangeClass objects.</p>\\n\\n<p>I need to find within which range a given value is.</p>\\n\\n<p>I can use some code like this:</p>\\n\\n<pre><code>for r in ragelist:\\n    if(value&gt;=r.address and value&lt;(r.address+r.size)):\\n        return r\\nreturn None\\n</code></pre>\\n\\n<p>But I think there is a faster way. Ranges have arbitrary size, but we can assume that they don\\'t overlap.</p>\\n\\n<p>Thank you. </p>\\n###Bot: You can use the bisect module to search for the appropriate range. First, you need to sort the rangelist based on the address of each range. Then, you can use the bisect_left function to find the index of the range that the value falls within. Here\\'s an example implementation:\\n\\n```\\nimport bisect\\n\\nclass RangeClass(object):\\n    def __init__(self, address, size):\\n        self.address = address\\n        self.size = size\\n\\n        # other attributes and methods...\\n\\ndef get_range(value, rangelist):\\n    rangelist.sort(key=lambda r: r.address)  # sort the rangelist by address\\n    i = bisect.bisect_left([r.address for r in rangelist], value)  # find the index of the range that the value falls within\\n    if i != len(rangelist) and value < rangelist[i].address + rangelist[i].size:\\n        return rangelist[i]  # return the range if the value falls within it\\n    else:\\n        return None  # return None if the value is outside all ranges\\n```\\n\\nThis implementation has a time complexity of O(log n) thanks to the binary search performed by bisect.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(train_data[32542].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "from utils import load_model, get_tokenizer, print_gpu_utilization, time_gpu\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = ['resume', 'eval_llama', 'llama', 'gpt2-small', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'][1] # or 'resume' or 'gpt2-medium' or 'gpt2-large' or 'gpt2-xl'\n",
    "out_dir = \"/home/li/basu_workspace/cptData/out/lb2_llama_instruct_0607-1556\"\n",
    "start = \"###User: Write a few words on Einstein.\\n\\n###Bot:\"## Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples =  3  # number of samples to draw\n",
    "max_new_tokens = 200 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda:1' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# learning block\n",
    "learning_block = True\n",
    "influence = 0.5\n",
    "\n",
    "## sampling\n",
    "break_at_eos = False\n",
    "eos_token_id = 2\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "model_type = 'llama' if 'llama' in init_from else 'gpt2'\n",
    "sampling = \"format\" # \"discrete\" or \"continuous\" or \"format\"\n",
    "\n",
    "\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "torch.set_default_dtype(ptdtype)\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# # model\n",
    "# model, model_args = load_model(model_type, out_dir, device, learning_block, influence, init_from)\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# with time_gpu('move model to device'):\n",
    "#     model.to(device)\n",
    "\n",
    "# print_gpu_utilization()\n",
    "\n",
    "# if compile:\n",
    "#     model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "\n",
    "# tokenizer\n",
    "encode, decode = get_tokenizer(model_type)\n",
    "                        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(idx, max_new_tokens, temperature=1.0, top_k=None, break_at_eos=False, eos_token_id=None):\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        idx_cond = idx if idx.size(1) <= 2048 else idx[:, -2048:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits = model(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        print(idx_next)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        if break_at_eos and idx_next.item() == eos_token_id:\n",
    "            print(\"breaking at eos\")\n",
    "            break\n",
    "\n",
    "    \n",
    "    return idx\n",
    "\n",
    "def sample(start):\n",
    "    start_ids = encode(start)\n",
    "    tkns = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            for k in range(num_samples):\n",
    "                with time_gpu('Time to generate'):\n",
    "                    print(\"Sample\", k+1, \"------------------------------------\")\n",
    "                    y = generate(tkns, max_new_tokens, temperature=temperature, top_k=top_k,\n",
    "                                       break_at_eos=break_at_eos, eos_token_id=eos_token_id)\n",
    "                                       \n",
    "                    print(decode(y[0].tolist()))\n",
    "                    print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        \n",
    "if sampling == \"discrete\":\n",
    "    # encode the beginning of the prompt\n",
    "    if start.startswith('FILE:'):\n",
    "        with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "            start = f.read()\n",
    "            sample(start)\n",
    "\n",
    "\n",
    "elif sampling == \"continuous\":\n",
    "    while True:\n",
    "        ## take input\n",
    "        print(\"Enter a sentence to continue:\")\n",
    "        start = str(input())\n",
    "        sample(start)\n",
    "        \n",
    "elif sampling == \"format\":\n",
    "    while True:\n",
    "        ## take input\n",
    "        print(\"Enter a sentence to continue:\")\n",
    "        start = str(input())\n",
    "        start = \"###User: \" + start + \"\\n\\n###Bot: \"\n",
    "        sample(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Learning Block True\n",
      "Iterations per epoch: 3659\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/li/basu_workspace/nanoGPT/test.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 212>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=192'>193</a>\u001b[0m model_args \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(n_layers\u001b[39m=\u001b[39mn_layers, n_heads\u001b[39m=\u001b[39mn_heads, n_embd\u001b[39m=\u001b[39mn_embd, block_size\u001b[39m=\u001b[39mblock_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=193'>194</a>\u001b[0m                   bias\u001b[39m=\u001b[39mbias, vocab_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dropout\u001b[39m=\u001b[39mdropout, learning_block\u001b[39m=\u001b[39mlearning_block) \u001b[39m# start with model_args from command line\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=196'>197</a>\u001b[0m \u001b[39m# model, model_args = load_model(model_type, out_dir, device, learning_block, influence, init_from)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=197'>198</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=198'>199</a>\u001b[0m \u001b[39m# with time_gpu(device, 'model to GPU'):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=209'>210</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=210'>211</a>\u001b[0m \u001b[39m## total number of parameters that requires grad\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=211'>212</a>\u001b[0m total_params \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters() \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mrequires_grad)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=212'>213</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtotal params with requires grad\u001b[39m\u001b[39m\"\u001b[39m, total_params\u001b[39m/\u001b[39m\u001b[39m1e6\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=214'>215</a>\u001b[0m \u001b[39m# initialize a GradScaler. If enabled=False scaler is a no-op\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torcheval.metrics.text import Perplexity\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from utils import load_model, Sampler, get_batch, configure_optimizers, time_gpu, get_pred_idxs\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "sample_start = \"The king exclaimed thou\"\n",
    "max_new_tokens=100\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # or 'resume' or 'gpt2-medium' or 'gpt2-large' or 'gpt2-xl' or 'eval_llama' or 'llama'\n",
    "# wandb logging\n",
    "wandb_log = False # disabled by default\n",
    "wandb_project = \"transformers\"\n",
    "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = 'openwebtext'\n",
    "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
    "batch_size = 32 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 2048\n",
    "# model\n",
    "n_layers = 12\n",
    "n_heads = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "torch.set_default_dtype(ptdtype)\n",
    "\n",
    "# learning block\n",
    "learning_block = False\n",
    "influence = 0.5\n",
    "\n",
    "## instruct\n",
    "data_type = None\n",
    "break_at_eos=False\n",
    "eos_token_id=1\n",
    "train_on_user_only = False\n",
    "\n",
    "## testing\n",
    "test_only = False\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "eval_interval = 5\n",
    "eval_iters = 40\n",
    "wandb_log = True # feel free to turn on\n",
    "wandb_project = 'learning-block'\n",
    "\n",
    "sample_start = \"###User: Write a few words on Einstein.\\n###Bot:\"\n",
    "max_new_tokens = 100\n",
    "\n",
    "wandb_run_name = 'lb2_llama_instruct' + '_' + time.strftime(\"%m%d-%H%M\") ## train_type,  model , dataset\n",
    "dataset = 'instruct2'\n",
    "init_from = 'llama'\n",
    "\n",
    "data_type = 'instruct'\n",
    "out_dir = '../cptData/out/' + wandb_run_name \n",
    "\n",
    "# only save checkpoints if the validation loss improves\n",
    "always_save_checkpoint = False\n",
    "\n",
    "# the number of examples per iter:\n",
    "# 1 batch_size * 32 grad_accum * 1024 tokens = 32,768 tokens/iter\n",
    "# shakespeare has 301,966 tokens, so 1 epoch ~= 9.2 iters\n",
    "batch_size = 2\n",
    "gradient_accumulation_steps = 32\n",
    "max_iters = 7500//batch_size\n",
    "\n",
    "learning_block = True\n",
    "device = 'cuda:1'\n",
    "\n",
    "learning_rate = 3e-4\n",
    "lr_decay_iters = 300\n",
    "decay_lr = True\n",
    "warmup_iters = 20\n",
    "\n",
    "compile = False\n",
    "\n",
    "break_at_eos = False\n",
    "eos_token_id = 2\n",
    "\n",
    "train_on_user_only = False\n",
    "\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "model_type = 'llama' if 'llama' in init_from else 'gpt2'\n",
    "\n",
    "## if torch version < 2 set compile to False\n",
    "if torch.__version__[0] == '1' and compile:\n",
    "    print(\"PyTorch version < 2.0, disabling compilation\")\n",
    "    compile = False\n",
    "\n",
    "print(\"Using Learning Block\", learning_block)\n",
    "\n",
    "# various inits, derived attributes, I/O setup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank # each process gets a different seed\n",
    "    assert gradient_accumulation_steps % torch.cuda.device_count() == 0\n",
    "    gradient_accumulation_steps //= torch.cuda.device_count()\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# poor man's data loader\n",
    "data_dir = os.path.join(os.path.dirname(os.path.realpath(\"__file__\")), 'data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "print(\"Iterations per epoch:\", train_data.shape[0] // tokens_per_iter)\n",
    "\n",
    "\n",
    "if data_type == 'instruct':\n",
    "    mask_train = np.memmap(os.path.join(data_dir, 'inp_shape_train.bin'), dtype=np.uint16, mode='r')\n",
    "    mask_val = np.memmap(os.path.join(data_dir, 'inp_shape_val.bin'), dtype=np.uint16, mode='r')\n",
    "    train_data = train_data.reshape(-1, block_size)\n",
    "    val_data = val_data.reshape(-1, block_size)\n",
    "    mask_train = mask_train.reshape(train_data.shape[0], -1)\n",
    "    mask_val = mask_val.reshape(val_data.shape[0], -1)\n",
    "\n",
    "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layers=n_layers, n_heads=n_heads, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout, learning_block=learning_block) # start with model_args from command line\n",
    "\n",
    "\n",
    "# model, model_args = load_model(model_type, out_dir, device, learning_block, influence, init_from)\n",
    "\n",
    "# with time_gpu(device, 'model to GPU'):\n",
    "#     model.to(device)\n",
    "\n",
    "# ## set requires grad to false for all layers except learning block\n",
    "\n",
    "# if learning_block:\n",
    "#     print(\"setting requires_grad=False for all layers except learning block\")\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.requires_grad:\n",
    "#             if \"learning_block\" not in name:\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "## total number of parameters that requires grad\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"total params with requires grad\", total_params/1e6, \"M\")\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = configure_optimizers(model, weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "# if init_from == 'resume':\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "# compile the model\n",
    "if compile:\n",
    "    with time_gpu(device, 'compiling model'):\n",
    "        print(\"compiling the model... (takes a ~minute)\")\n",
    "        model = torch.compile(model) # requires PyTorch 2.0\n",
    "\n",
    "# wrap model into DDP container\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])\n",
    "\n",
    "sampler = Sampler(model_name = model_type, start = sample_start, device = device)\n",
    "\n",
    "perplexity = Perplexity()\n",
    "perplexity.to(device)\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Sampling from trained model\")\n",
    "    sampler.generate(model, max_new_tokens=max_new_tokens, break_at_eos = break_at_eos,eos_token_id = eos_token_id) \n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y, mask, pred_idxs = get_batch(split, block_size, batch_size, device_type, device, train_data, val_data, \n",
    "                                   data_type = data_type, mask_train = mask_train, mask_val = mask_val, train_on_user_only = train_on_user_only)\n",
    "                \n",
    "            with ctx:\n",
    "                logits = model(X, mask = mask)\n",
    "                                \n",
    "            if train_on_user_only:\n",
    "                logits = logits.gather(1, torch.tensor(pred_idxs, device=device).unsqueeze(2).repeat(1,1,logits.size(-1))).squeeze(2)\n",
    "                Y = Y.gather(1, torch.tensor(pred_idxs, device=device)).squeeze(1)\n",
    "\n",
    "            perplexity.update(logits, Y)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=-1)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "        print(f\"perplexity on {split} split: {perplexity.compute():.3f}\")\n",
    "        perplexity.reset()\n",
    "        \n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only:\n",
    "    print(\"Testing the model only\")\n",
    "    with time_gpu(device,'Ealuate'):\n",
    "        losses = estimate_loss()\n",
    "        \n",
    "    print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    exit()\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "# logging\n",
    "if wandb_log and master_process:\n",
    "    import wandb\n",
    "    wandb_api_key = \"84742742b66deb0de22b5dfec52ec1f23a539d9b\"\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, entity='basujindal123',config=config)\n",
    "\n",
    "# training loop\n",
    "X, Y, mask, pred_idxs = get_batch('train', block_size, batch_size, device_type, device, train_data, val_data,\n",
    "                       data_type = data_type, mask_train = mask_train, mask_val = mask_val, train_on_user_only = train_on_user_only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, mask, pred_idxs = get_batch('train', block_size, 1, device_type, device, train_data, val_data,\n",
    "                       data_type = data_type, mask_train = mask_train, mask_val = mask_val, train_on_user_only = train_on_user_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2047, 2047]), torch.Size([1, 2047]), torch.Size([1, 2047]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape, X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "torch.Size([2, 2047]) torch.Size([2, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 128]) torch.Size([2, 32, 2047, 2047])\n",
      "Training Step 2.682549238204956\n",
      "GPU memory used: 0.0\n",
      "  Total  | Reserved |Allocated|   Max   \n",
      " 39.41GB | 37.95GB | 36.33GB | 37.11GB \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 1; 39.41 GiB total capacity; 36.96 GiB already allocated; 284.50 MiB free; 37.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/li/basu_workspace/nanoGPT/test.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mwith\u001b[39;00m ctx:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mprint\u001b[39m(X\u001b[39m.\u001b[39mshape, mask\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     logits \u001b[39m=\u001b[39m model(X, mask \u001b[39m=\u001b[39;49m mask)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m train_on_user_only:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bece228-3.ucsd.edu/home/li/basu_workspace/nanoGPT/test.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mgather(\u001b[39m1\u001b[39m, torch\u001b[39m.\u001b[39mtensor(pred_idxs, device\u001b[39m=\u001b[39mdevice)\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,logits\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/basu/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/basu_workspace/nanoGPT/llamaModel.py:108\u001b[0m, in \u001b[0;36mLLaMA.forward\u001b[0;34m(self, idx, max_seq_length, input_pos, mask)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m input_pos \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# proxy for use_cache=False\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mh:\n\u001b[0;32m--> 108\u001b[0m         x, _ \u001b[39m=\u001b[39m block(x, rope, mask, max_seq_length)\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkv_caches:\n",
      "File \u001b[0;32m~/anaconda3/envs/basu/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/basu_workspace/nanoGPT/llamaModel.py:195\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, rope, mask, max_seq_length, input_pos, kv_cache)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    187\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    188\u001b[0m     x: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m     kv_cache: Optional[KVCache] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    194\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, Optional[KVCache]]:\n\u001b[0;32m--> 195\u001b[0m     h, new_kv_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrms_1(x), rope, mask, max_seq_length, input_pos, kv_cache)\n\u001b[1;32m    196\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m h\n\u001b[1;32m    197\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrms_2(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/basu/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/basu_workspace/nanoGPT/llamaModel.py:276\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x, rope, mask, max_seq_length, input_pos, kv_cache)\u001b[0m\n\u001b[1;32m    274\u001b[0m mask \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    275\u001b[0m \u001b[39mprint\u001b[39m(q\u001b[39m.\u001b[39mshape, k\u001b[39m.\u001b[39mshape, v\u001b[39m.\u001b[39mshape, mask\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 276\u001b[0m y \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mscaled_dot_product_attention(q, k, v, attn_mask\u001b[39m=\u001b[39;49mmask, dropout_p\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m)\n\u001b[1;32m    278\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(B, T, C)  \u001b[39m# re-assemble all head outputs side by side\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[39m# output projection\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 1; 39.41 GiB total capacity; 36.96 GiB already allocated; 284.50 MiB free; 37.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
    "iter_num_resume = iter_num\n",
    "        \n",
    "        \n",
    "print(\"Training\")\n",
    "for iter_num in range(iter_num_resume, max_iters+1):\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "    with time_gpu(device, 'Training Step'):\n",
    "        \n",
    "        for micro_step in range(gradient_accumulation_steps):\n",
    "            if ddp:\n",
    "                model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "            with ctx:\n",
    "                print(X.shape, mask.shape)\n",
    "                logits = model(X, mask = mask)\n",
    "                \n",
    "            if train_on_user_only:\n",
    "                logits = logits.gather(1, torch.tensor(pred_idxs, device=device).unsqueeze(2).repeat(1,1,logits.size(-1))).squeeze(2)\n",
    "                Y = Y.gather(1, torch.tensor(pred_idxs, device=device)).squeeze(1)\n",
    "\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=-1)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "                \n",
    "            # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "            X, Y, mask, pred_idxs = get_batch('train', block_size, batch_size, device_type, device, train_data, val_data,\n",
    "                                   data_type = data_type, mask_train = mask_train, mask_val = mask_val, train_on_user_only = train_on_user_only)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "        # clip the gradient\n",
    "        if grad_clip != 0.0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = np.ones((len(encoded), seq_len), dtype=np.int32)*2\n",
    "j = 0\n",
    "k = 0\n",
    "sen_lens = []\n",
    "l = []\n",
    "for i in encoded:\n",
    "    if k + len(i) > seq_len:\n",
    "        sen_lens.append(l)\n",
    "        l = [len(i)]\n",
    "        j+=1\n",
    "        k=len(i)\n",
    "    else:\n",
    "        k+=len(i)\n",
    "        l.append(len(i))\n",
    "    comb[j, k-len(i):k] = i\n",
    "\n",
    "for i in range(len(comb)):\n",
    "    if np.all(comb[i] == 2):\n",
    "        num_sen = i-1\n",
    "        break\n",
    "\n",
    "comb = comb[:num_sen]\n",
    "\n",
    "max_len = max([len(i) for i in sen_lens])\n",
    "## pad sen_lens with zeros\n",
    "sen_lens = [i + [0]*(max_len - len(i)) for i in sen_lens]\n",
    "sen_lens = [item for sublist in sen_lens for item in sublist]\n",
    "\n",
    "train_frac = 0.9\n",
    "train_ids = comb[:int(train_frac*len(comb))]\n",
    "val_ids = comb[int(train_frac*len(comb)):]\n",
    "\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "sen_lens = np.array(sen_lens, dtype=np.uint16)\n",
    "train_ids.tofile('/home/li/basu_workspace/nanoGPT/data/dolly/train.bin')\n",
    "val_ids.tofile( '/home/li/basu_workspace/nanoGPT/data/dolly/val.bin')\n",
    "sen_lens.tofile('/home/li/basu_workspace/nanoGPT/data/dolly/sen_lens.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/li/basu_workspace/nanoGPT/data/databricks-dolly-15k.jsonl') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "data = [json.loads(line) for line in data]\n",
    "data_cleaned  = [\"User: \" + instruct['instruction'] + \"\\nBot: \" + instruct['response'] for instruct in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = []\n",
    "for sentence in data_cleaned:\n",
    "    encoded.append(enc(sentence))\n",
    "assert len (encoded) == len(data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(encoded[i]) for i in range(len(encoded))]\n",
    "sorted_lens = sorted(lens)\n",
    "pl = plt.hist(sorted_lens[:-200], bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import safetensors\n",
    "from safetensors.torch import save_model\n",
    "\n",
    "save_model(model, 'my_model.safetensors')\n",
    "\n",
    "# tensors = {}\n",
    "# with safe_open(\"model.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "#    for key in f.keys():\n",
    "#        tensors[key] = f.get_tensor(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import safetensors\n",
    "safetensors.torch.load_model(model, 'my_model.safetensors')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
